{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15f085-259f-4074-bd31-ab7a8720e801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import socketio\n",
    "sio = socketio.AsyncClient()\n",
    "\n",
    "file1 = \"ru_lock_unlock_normal1_simple.log\"\n",
    "file2 = \"ru_lock_unlock_dpd_hw_fault_simple.log\"\n",
    "\n",
    "await sio.connect('http://127.0.0.1:8000', namespaces=['/TextAnalysis/FileContainer', '/TextAnalysis/TextFileCompare'])\n",
    "await sio.emit('new_file', [f'D:\\\\Projects\\\\ericsson_flow\\\\new_files\\\\{file1}'], namespace='/TextAnalysis/FileContainer')\n",
    "await asyncio.sleep(0.2)\n",
    "await sio.emit('load_config', 'D:\\\\projects\\\\ericsson_flow\\\\new_files\\\\6419config.ecfg', namespace='/TextAnalysis/FileContainer')\n",
    "await asyncio.sleep(10)\n",
    "await sio.emit('new_file', [f'D:\\\\Projects\\\\ericsson_flow\\\\new_files\\\\{file2}'], namespace='/TextAnalysis/FileContainer')\n",
    "await asyncio.sleep(0.2)\n",
    "await sio.emit('load_config', 'D:\\\\projects\\\\ericsson_flow\\\\new_files\\\\6419config.ecfg', namespace='/TextAnalysis/FileContainer')\n",
    "await asyncio.sleep(10)\n",
    "await sio.emit('exec', {'first_file_namespace': f'/TextAnalysis/FileContainer/{file1}', 'second_file_namespace': f'/TextAnalysis/FileContainer/{file2}'}, namespace='/TextAnalysis/TextFileCompare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb27fd-377f-4b2f-a56d-b7f99749cbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sio1 = socketio.AsyncClient()\n",
    "await sio1.connect('http://127.0.0.1:8000', namespaces=[\n",
    "'/TextAnalysis/FileContainer',\n",
    "'/TextAnalysis/TextFileCompare',\n",
    "'/TextAnalysis/FileContainer/ru_lock_unlock_normal1_simple.log/TextFileFunction/SearchFunction/txlProcBranch0',\n",
    "'/TextAnalysis/FileContainer/ru_lock_unlock_dpd_hw_fault_simple.log/TextFileFunction/SearchFunction/txlProcBranch0'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b7ebe-2c69-452b-92a6-b819d607f3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1 = {}\n",
    "model2 = {}\n",
    "\n",
    "def set_data1(model):\n",
    "    model1.update(model)\n",
    "def set_data2(model):\n",
    "    model2.update(model)\n",
    "await sio1.emit('model', namespace='/TextAnalysis/FileContainer/ru_lock_unlock_normal1_simple.log/TextFileFunction/SearchFunction/txlProcBranch0', callback=set_data1)\n",
    "await asyncio.sleep(0.2)\n",
    "await sio1.emit('model', namespace='/TextAnalysis/FileContainer/ru_lock_unlock_dpd_hw_fault_simple.log/TextFileFunction/SearchFunction/txlProcBranch0', callback=set_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5c649-c93d-4e58-9400-7952d83b5b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "string = '       air_mongoose com_ericsson_trithread INFO cpu_id  process txlProcBranch fileAndLine spiMaster cc  msg Warning Slow response to SPI_SEND_REQ for device paCtrlDevice   took ms '\n",
    "re.sub(' '+\"+\", ' ', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514940e7-4ba2-401d-91fe-565b8b3b3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = 'Power measurement'\n",
    "for item in model1['res_clean_lines']:\n",
    "    if s in item:\n",
    "        print('model1', item)\n",
    "        \n",
    "for item in model2['res_clean_lines']:\n",
    "    if s in item:\n",
    "        print('model2', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f4edf-0413-4bf3-ac33-ac63a293845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t[14:56:47.927884604] (+0.001409563) air6419_mongoose com_ericsson_trithread:INFO: { cpu_id = 2 }, { process = \"WorkerTaskAas\", fileAndLine = \"algAasHelper.cc:659\", msg = \"Missing subbandMappingExt param, using default fullband instead.\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26446a-5c23-406a-9ba8-3ada3a5745ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2['res_clean_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1768f2-5582-488b-882b-3c5bad989fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class A(object):\n",
    "    def __init__(self):\n",
    "        self.a = 1\n",
    "        self.b = 2\n",
    "        \n",
    "    def __getattribute__(self, obj):\n",
    "        print(obj)\n",
    "        return object.__getattribute__(self, obj)\n",
    "    \n",
    "c = A()\n",
    "c.a * c.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68ae71-a31a-43eb-b5c8-5858319c55e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from asyncio import get_event_loop\n",
    "from text_analysis import *\n",
    "\n",
    "# loop = get_event_loop()\n",
    "# loop.run_until_complete(TextAnalysisModel('parallel'))\n",
    "\n",
    "web.run_app(app, host=\"127.0.0.1\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bed893-12aa-40c5-8aa0-685e15d37007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as dp\n",
    "\n",
    "str(dp('221210-12:40:51', yearfirst=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843578f-c129-4057-abfe-4e4633cc13e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if '2022-12-11 01:12:21.105110' > '2022-12-11 01:12:22':\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d155b-32e7-4219-8d01-047fc736d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from parse import parse\n",
    "\n",
    "str1 = '230228-16:19:06+0100 172.21.11.66 23.0b MSRBS_NODE_MODEL_22.Q2_566.28125.116_3317 stopfile=/tmp/11737'\n",
    "str2 = 'BXP_7:     769,dl-1  disabled        false    essFdd150_Id2       B     dl    2162500         BandI      0,       0    469            469     SETUP     SETUP'\n",
    "# exp_extract = '{}MSRBS_NODE_MODEL_{version}_{}'\n",
    "exp_extract = '{bxp}:{:s}{carrierId}{:s}{Enabled}{:s}{RealRelease}{:s}{carrierType}{:s}{rfPort}{:s}dl{:s}{Frequency}{:s}{Band}{:s}{Arfcn_min}{:s}{Arfcn_max}{:s}{Power}{:s}{ReservedPower}{:s}{tr}{:s}{bdconf}'\n",
    "r_extract = parse(exp_extract, str2)\n",
    "print(r_extract.named)\n",
    "# print(r_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc08b0-17cb-4a11-bbc8-14f46f0f8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for index in self.text_file.data['test'].res_lines:\n",
    "    lines.append(self.text_file.lines[index])\n",
    "\n",
    "with open('D:\\\\projects\\\\ericsson_flow\\\\new_files\\\\ru_lock_unlock_normal2_simple.log', 'w', encoding='utf-8') as f:\n",
    "    f.write(''.join(lines))\n",
    "    \n",
    "# jesd|dfe|radiosw|rProxyMedian.cc # (txlProcBranch0|TxBranchCtrl0).*event com_ericsson_trithread:INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da1f0d-de24-450c-b58b-010130d4d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = self.text_file.data['dl'].res_key_value\n",
    "file = pd.DataFrame(pd.Series(data['Band']['value']), columns=['Band'])\n",
    "\n",
    "file['rfPort'] = pd.Series(data['rfPort']['value'])\n",
    "file['rfPort'] = 'Port_' + file['rfPort']\n",
    "file['carrierType'] = pd.Series(data['carrierType']['value'])\n",
    "file['Power'] = pd.Series(data['Power']['value'])\n",
    "\n",
    "result = {\n",
    "            'SiteName': self.text_file.file_name.split('_radio4480')[0],\n",
    "            'Version': self.text_file.data['version'].res_key_value['version']['value'][0],\n",
    "            'RadioQuantity': len(self.text_file.data['device'].res_lines)\n",
    "         }\n",
    "for band,port,carrier,power in file.values:\n",
    "    result[band+'_'+port+'_carrier_config'] = carrier\n",
    "    result[band+'_'+port+'_output_power_config'] = power\n",
    "\n",
    "self.result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07042a0-ce72-48ff-8dd7-11415ec32f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('C:\\\\Users\\\\MSI-NB\\\\Downloads\\\\6651_config_Denmark_68_Nodes\\\\KaiHong_4480config_origin_data.csv')\n",
    "data = data.fillna('')\n",
    "columns = ['SiteName', 'Bxp', 'Version']\n",
    "tmpl = sorted(list(set(data.columns).difference(set(columns))))\n",
    "band1 = []\n",
    "band3 = []\n",
    "for c in tmpl:\n",
    "    if 'BandIII_' in c:\n",
    "        band3.append(c)\n",
    "    elif 'BandI_' in c:\n",
    "        band1.append(c)\n",
    "        \n",
    "band1.extend(band3)\n",
    "columns = band1\n",
    "total = len(data)\n",
    "res = data.groupby(columns).size().reset_index(name='RadioAmount')\n",
    "for index,item in enumerate(json.loads(res.to_json(orient='records'))):\n",
    "    band1_dbm = []\n",
    "    band3_dbm = []\n",
    "    total_dbm = 0\n",
    "    for key in item.keys():\n",
    "        if ('output_power_BandIII' in key) & (item[key] != ''):\n",
    "            band3_dbm.append(item[key])\n",
    "        elif ('output_power_BandI' in key) & (item[key] != ''):\n",
    "            band1_dbm.append(item[key])\n",
    "    band1_dbm = sum(band1_dbm) if len(band1_dbm) > 0 else 0\n",
    "    band3_dbm = sum(band3_dbm) if len(band3_dbm) > 0 else 0\n",
    "    total_dbm = band1_dbm + band3_dbm\n",
    "    res.loc[index, ['BandI_dBm', 'BandIII_dBm', 'BandI+BandIII_dBm']] = [band1_dbm,band3_dbm,total_dbm]\n",
    "res['Percentage'] = res['RadioAmount'] / total\n",
    "res.to_csv('C:\\\\Users\\\\MSI-NB\\\\Downloads\\\\6651_config_Denmark_68_Nodes\\\\6651_config_Sweden_204_Nodes_config_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfaa13e-e71e-4a3c-8078-c27a7cdb7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = self.text_analysis.batch_statistic.table['table'].values\n",
    "data = pd.DataFrame()\n",
    "for index, item in enumerate(items):\n",
    "    print(index)\n",
    "    try:\n",
    "        tmp = pd.DataFrame(item)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        continue\n",
    "    data = data.append(tmp, ignore_index=True)\n",
    "    \n",
    "columns = ['SiteName', 'Bxp', 'Version']\n",
    "tmpl = sorted(list(set(data.columns).difference(set(columns))))\n",
    "band1 = []\n",
    "band3 = []\n",
    "for c in tmpl:\n",
    "    if 'BandIII_' in c:\n",
    "        band3.append(c)\n",
    "    elif 'BandI_' in c:\n",
    "        band1.append(c)\n",
    "        \n",
    "columns.extend(band1)\n",
    "columns.extend(band3)\n",
    "data = data.reindex(columns=columns)\n",
    "data.to_csv('C:\\\\Users\\\\MSI-NB\\\\Downloads\\\\6651_config_Denmark_68_Nodes\\\\KaiHong_4480config_origin_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532a7d2-c2ec-41ae-806b-0a25c956fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = self.text_file.data['dl'].res_key_value\n",
    "file = pd.DataFrame(pd.Series(data['Band']['value']), columns=['Band'])\n",
    "\n",
    "file['bxp'] = pd.Series(data['bxp']['value'])\n",
    "file['rfPort'] = pd.Series(data['rfPort']['value'])\n",
    "file['rfPort'] = 'Port_' + file['rfPort']\n",
    "file['carrierType'] = pd.Series(data['carrierType']['value'])\n",
    "file['Power'] = pd.Series(data['Power']['value'])\n",
    "\n",
    "result = []\n",
    "for bxp in set(file['bxp'].values):\n",
    "    item = {\n",
    "                'SiteName': self.text_file.file_name.split('.log')[0],\n",
    "                'Bxp': bxp,\n",
    "                'Version': self.text_file.data['version'].res_key_value['version']['value'][0]\n",
    "             }\n",
    "    tmp = file.loc[(file['bxp'] == bxp), :].reset_index(drop=True)\n",
    "    for band,bxp,port,carrier,power in tmp.values:\n",
    "        if band in ['BandI', 'BandIII']:\n",
    "            cc = band+'_'+port+'_carrier_config'\n",
    "            pc = band+'_'+port+'_output_power_config'\n",
    "            dBm = 'output_power_' + band+'_'+port + '_dBm'\n",
    "            if cc in item:\n",
    "                item[cc] = carrier + ' + ' + item[cc] if carrier < item[cc] else item[cc] + ' + ' + carrier\n",
    "                item[pc] = power + ' + ' + item[pc] if carrier < item[cc] else item[pc] + ' + ' + power\n",
    "                item[dBm] = item[dBm] + round(10**(int(power)/100)/1000, 1)\n",
    "            else:\n",
    "                item[cc] = carrier\n",
    "                item[pc] = power\n",
    "                item[dBm] = round(10**(int(power)/100)/1000, 1)\n",
    "    if len(item.keys())> 3:\n",
    "        result.append(item)\n",
    "\n",
    "self.result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a20888-6166-4e92-a73a-0844aba40c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\projects\\\\ericsson_flow\\\\CC summary V2,0.xlsx'\n",
    "output_path = 'D:\\\\projects\\\\ericsson_flow\\\\output'\n",
    "\n",
    "sheet_names = pd.read_excel(file_path, sheet_name=None).keys()\n",
    "sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bcf46-dc83-4475-b363-8afa60666fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_excel(file_path, sheet_name='Claro_4480_cc_SW', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf6166-9977-4946-ba30-3b460b7be537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'D:\\\\projects\\\\ericsson_flow\\\\CC summary V2,0.xlsx'\n",
    "output_path = 'D:\\\\projects\\\\ericsson_flow\\\\output'\n",
    "\n",
    "print('Loading file...')\n",
    "sheet_names = pd.read_excel(file_path, sheet_name=None).keys()\n",
    "for sheet_name in sheet_names:\n",
    "    print(f'Start Handle {sheet_name}...')\n",
    "    data = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "    result = []\n",
    "    x= list(data['SerialNo'].values)\n",
    "    sns = sorted(set(x), key=x.index)\n",
    "    for sn in sns:\n",
    "        radio = data.loc[(data['SerialNo'] == sn)&(data['MaxTxPower_br'].notna()), :].reset_index(drop=True)\n",
    "        if len(radio) == 0:\n",
    "            continue\n",
    "        item = {'SerialNo': sn, 'SwPack': list(radio['SwPack'].values)[0]}\n",
    "        for port,band,power in radio[['rfPortId', 'freqBand', 'MaxTxPower_br']].values:\n",
    "            if band in [1,3]:\n",
    "                item['Band'+str(int(band))+'_Port'+port+'_MaxTxPower_br'] = power\n",
    "\n",
    "        band1_power = []\n",
    "        band3_power = []\n",
    "        for key in item.keys():\n",
    "            if 'Band1' in key:\n",
    "                band1_power.append(item[key])\n",
    "            if 'Band3' in key:\n",
    "                band3_power.append(item[key])\n",
    "\n",
    "        item['Band1_MaxTxPower_br'] = sum(band1_power) if len(band1_power) > 0 else 0\n",
    "        item['Band3_MaxTxPower_br'] = sum(band3_power) if len(band3_power) > 0 else 0\n",
    "        item['Band1+Band3_MaxTxPower_br'] = item['Band1_MaxTxPower_br'] + item['Band3_MaxTxPower_br']\n",
    "        result.append(item)\n",
    "    data = ''\n",
    "    del data\n",
    "    origin_data = pd.DataFrame(result)\n",
    "    origin_data.to_csv(f'{output_path}\\\\{sheet_name}_Origin_Data.csv', index=False)\n",
    "\n",
    "    origin_data = origin_data.fillna('')\n",
    "    total = len(origin_data)\n",
    "    summary = origin_data.groupby(list(origin_data.columns[2:])).size().reset_index(name='RadioAmount')\n",
    "    summary['Percentage'] = summary['RadioAmount'] / total\n",
    "    summary.to_csv(f'{output_path}\\\\{sheet_name}_Summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe86ef3-05cd-443e-8b7e-df84fcac089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script name: my_script.py\n",
    "Author: DengJun Lei(EEEINLD)\n",
    "Date created: March 28, 2023\n",
    "Description: Collect the configuration information for multiple 4480 radios.\n",
    "Process: \n",
    "        1.Extract file returned by 'carrierListHandler print table' command.\n",
    "        2.Extract (version carrierType rfPort dl/ul Band Power)\n",
    "        3.Batch extraction and output to a table. \n",
    "        4.Export origin data based on a single radio.\n",
    "        5.Export summary information based on the configuration information of all radios.\n",
    "\"\"\"\n",
    "\n",
    "###################### User Define ###############################\n",
    "# Files directory Location\n",
    "self.dir_path = 'C:\\\\Users\\\\MSI-NB\\\\Downloads\\\\6651_config_Denmark_68_Nodes\\\\6651_config_Denmark_68_Nodes'\n",
    "# Config Location\n",
    "self.config_path = 'D:\\\\projects\\\\ericsson_flow\\\\configs\\\\4480_Configure_Statistic_Config.ecfg'\n",
    "# Output Location\n",
    "self.output_path = 'D:\\\\projects\\\\ericsson_flow\\\\output'\n",
    "# Save name\n",
    "self.save_name = '6651_config_Denmark_68_Nodes'\n",
    "\n",
    "###################### Batch handle file, according to config ###############################\n",
    "table = await self.batch_handle(self.dir_path, self.config_path)\n",
    "table = table.drop(['search_atoms', 'chart_atoms', 'statistic_atoms'], axis=1)\n",
    "\n",
    "###################### Export origin data ###############################\n",
    "items = table['table'].values\n",
    "data = pd.DataFrame()\n",
    "for index, item in enumerate(items):\n",
    "    try:\n",
    "        tmp = pd.DataFrame(item)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        continue\n",
    "    data = data.append(tmp, ignore_index=True)\n",
    "    \n",
    "columns = ['SiteName', 'Bxp', 'Version']\n",
    "tmpl = sorted(list(set(data.columns).difference(set(columns))))\n",
    "band1 = []\n",
    "band3 = []\n",
    "for c in tmpl:\n",
    "    if 'BandIII_' in c:\n",
    "        band3.append(c)\n",
    "    elif 'BandI_' in c:\n",
    "        band1.append(c)\n",
    "        \n",
    "columns.extend(band1)\n",
    "columns.extend(band3)\n",
    "data = data.reindex(columns=columns)\n",
    "data.to_csv(f'{self.output_path}//{self.save_name}_origin_data.csv', index=False)\n",
    "await self.on_console(msg='Export Origin Data Finish!')\n",
    "\n",
    "###################### Export summary data ###############################\n",
    "data = data.fillna('')\n",
    "columns = ['SiteName', 'Bxp', 'Version']\n",
    "tmpl = sorted(list(set(data.columns).difference(set(columns))))\n",
    "band1 = []\n",
    "band3 = []\n",
    "for c in tmpl:\n",
    "    if 'BandIII_' in c:\n",
    "        band3.append(c)\n",
    "    elif 'BandI_' in c:\n",
    "        band1.append(c)\n",
    "        \n",
    "band1.extend(band3)\n",
    "columns = band1\n",
    "total = len(data)\n",
    "res = data.groupby(columns).size().reset_index(name='RadioAmount')\n",
    "for index,item in enumerate(json.loads(res.to_json(orient='records'))):\n",
    "    band1_dbm = []\n",
    "    band3_dbm = []\n",
    "    total_dbm = 0\n",
    "    for key in item.keys():\n",
    "        if ('output_power_BandIII' in key) & (item[key] != ''):\n",
    "            band3_dbm.append(item[key])\n",
    "        elif ('output_power_BandI' in key) & (item[key] != ''):\n",
    "            band1_dbm.append(item[key])\n",
    "    band1_dbm = sum(band1_dbm) if len(band1_dbm) > 0 else 0\n",
    "    band3_dbm = sum(band3_dbm) if len(band3_dbm) > 0 else 0\n",
    "    total_dbm = band1_dbm + band3_dbm\n",
    "    res.loc[index, ['BandI_dBm', 'BandIII_dBm', 'BandI+BandIII_dBm']] = [band1_dbm,band3_dbm,total_dbm]\n",
    "res['Percentage'] = res['RadioAmount'] / total\n",
    "res.to_csv(f'{self.output_path}//{self.save_name}_summary.csv', index=False)\n",
    "await self.on_console(msg='Export Summary Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848a8a4-f080-49cf-bfbd-22ed2d9a0c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Collect the configuration information for 4480 radios xslx.\n",
    "\n",
    "Author: DengJun Lei(EEEINLD)\n",
    "CreateTime: 2023.3.20\n",
    "ChangeTime: 2023.3.30\n",
    "Version: 1.1\n",
    "\"\"\"\n",
    "\n",
    "###################### User Define ###############################\n",
    "# Files directory Location\n",
    "self.xslx_path = 'D:\\\\projects\\\\ericsson_flow\\\\CC summary V2,0.xlsx'\n",
    "# Output Location\n",
    "self.output_path = 'D:\\\\projects\\\\ericsson_flow\\\\output'\n",
    "\n",
    "print('Loading file...')\n",
    "sheet_names = pd.read_excel(self.xslx_path, sheet_name=None).keys()\n",
    "for sheet_name in sheet_names:\n",
    "    print(f'Start Handle {sheet_name}...')\n",
    "    data = pd.read_excel(self.xslx_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "    \n",
    "    ###################### Export origin data ###############################\n",
    "    result = []\n",
    "    x= list(data['SerialNo'].values)\n",
    "    sns = sorted(set(x), key=x.index)\n",
    "    for sn in sns:\n",
    "        radio = data.loc[(data['SerialNo'] == sn)&(data['MaxTxPower_br'].notna()), :].reset_index(drop=True)\n",
    "        if len(radio) == 0:\n",
    "            continue\n",
    "        item = {'SerialNo': sn, 'SwPack': list(radio['SwPack'].values)[0]}\n",
    "        for port,band,power in radio[['rfPortId', 'freqBand', 'MaxTxPower_br']].values:\n",
    "            if band in [1,3]:\n",
    "                item['Band'+str(int(band))+'_Port'+port+'_MaxTxPower_br'] = power\n",
    "\n",
    "        band1_power = []\n",
    "        band3_power = []\n",
    "        for key in item.keys():\n",
    "            if 'Band1' in key:\n",
    "                band1_power.append(item[key])\n",
    "            if 'Band3' in key:\n",
    "                band3_power.append(item[key])\n",
    "\n",
    "        item['Band1_MaxTxPower_br'] = sum(band1_power) if len(band1_power) > 0 else 0\n",
    "        item['Band3_MaxTxPower_br'] = sum(band3_power) if len(band3_power) > 0 else 0\n",
    "        item['Band1+Band3_MaxTxPower_br'] = item['Band1_MaxTxPower_br'] + item['Band3_MaxTxPower_br']\n",
    "        result.append(item)\n",
    "    data = ''\n",
    "    del data\n",
    "    origin_data = pd.DataFrame(result)\n",
    "    origin_data.to_csv(f'{self.output_path}\\\\{sheet_name}_Origin_Data.csv', index=False)\n",
    "    \n",
    "    ###################### Export summary data ###############################\n",
    "    origin_data = origin_data.fillna('')\n",
    "    total = len(origin_data)\n",
    "    summary = origin_data.groupby(list(origin_data.columns[2:])).size().reset_index(name='RadioAmount')\n",
    "    summary['Percentage'] = summary['RadioAmount'] / total\n",
    "    summary.to_csv(f'{self.output_path}\\\\{sheet_name}_Summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf33fa-32df-4e39-a42b-6d62b3949088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script name: DOT4455_Abnormal_Analyze.escp\n",
    "Author: DengJun Lei(EEEINLD)\n",
    "Date created: April 11, 2023\n",
    "Description: Batch analyze DOT4455 abnormal keywords and group them based on RU, RdPort, and Abnormal.\n",
    "\"\"\"\n",
    "\n",
    "###################### User Define ###############################\n",
    "group = ['Root','RU','RP','Abnormal']\n",
    "# Files directory Location\n",
    "self.dir_path = 'D:/projects/ericsson_flow/new_files/low_tx_power_log0331'\n",
    "self.exps = [\n",
    "                {'name': 'DPD errorCode 0x340a', 'exp': '{RU}: OK {} {timestampd} {timestampt} {}: DPD errorCode 0x340a{}'},\n",
    "                {'name': 'Low tx Power', 'exp': '{RU}: OK {} {timestampd} {timestampt} {}: Low tx Power{}'}\n",
    "            ]\n",
    "self.end_marker = '{}Done RdId:{RI}, RdPort:{RP}{}'\n",
    "origin_data_output = 'D:/projects/ericsson_flow/new_files/OriginData.csv'\n",
    "root_name = 'DOT4455'\n",
    "pixel_width = 5000\n",
    "\n",
    "###################### Execution area ###############################\n",
    "await self.on_console(msg='Script running...')\n",
    "# extract keywords and generate origin data\n",
    "result = pd.DataFrame()\n",
    "for file_name in iterate_files_in_directory(self.dir_path):\n",
    "    path = f'{self.dir_path}\\\\{file_name}'\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    rd_port = []\n",
    "    for line in lines:\n",
    "        for exp in self.exps:\n",
    "            r = parse(exp['exp'], line)\n",
    "            if r is not None:\n",
    "                rd_port.append({'IP': file_name, 'RU': r['RU'], 'Abnormal': exp['name'], 'timestamp': convert_datetime_timestamp(r['timestampd'] + ' ' + r['timestampt'])})\n",
    "        if parse(self.end_marker, line) is not None:\n",
    "            tmp = pd.DataFrame(rd_port)\n",
    "            tmp['RP'] = parse(self.end_marker, line)['RP']\n",
    "            result = result.append(tmp).reset_index(drop=True)\n",
    "            rd_port = []\n",
    "    await self.on_console(msg=f'Finish {file_name}.')\n",
    "result['Root'] = root_name\n",
    "result['RP'] = result['RP'].astype(str)\n",
    "result = result.loc[:, ['Root','IP','RU','RP','Abnormal','timestamp']]\n",
    "result.to_csv(origin_data_output, index=False)\n",
    "\n",
    "# result = pd.read_csv('D:\\\\projects\\\\ericsson_flow\\\\new_files\\\\OriginData.csv')\n",
    "# result['RP'] = result['RP'].astype(str)\n",
    "\n",
    "# organize the required data.\n",
    "items = []\n",
    "grouped = result.groupby(group)\\\n",
    "    .apply(lambda x: x[['IP', 'timestamp']].assign(api='that.textAnalysisView.fileContainerView.controlNewFile([\"'+self.dir_path+'/'+x['IP']+'\"])').to_dict('records'))\\\n",
    "    .reset_index(name='data')\n",
    "group.append('data')\n",
    "for p in grouped[group].values:\n",
    "    groups = p[0: -1]\n",
    "    data = p[-1]\n",
    "    items.append({'path': list(groups), 'graph_type': 'ScatterPlot', 'start_x': min(pd.DataFrame(data)['timestamp'].values), 'end_x': max(pd.DataFrame(data)['timestamp'].values), 'elements': data})\n",
    "\n",
    "start_x = []\n",
    "end_x = []\n",
    "for item in items:\n",
    "    start_x.append(item['start_x'])\n",
    "    end_x.append(item['end_x'])\n",
    "\n",
    "graphs = []\n",
    "# define xaxis\n",
    "graphs.append({\n",
    "    'type': 'XAxis',\n",
    "    'width': pixel_width,\n",
    "    'lower_bound': min(start_x),\n",
    "    'upper_bound': max(end_x),\n",
    "    'tick_format_func': 'formatTimestamp'\n",
    "})\n",
    "\n",
    "# define IndentedTree\n",
    "global_inter = linear_scale([min(start_x), max(end_x)], [0, pixel_width])\n",
    "tree = IndentedTree('', 0, pixel_width, pixel_width, items, global_inter)\n",
    "graphs.append({'id': tree.id, 'type': tree.type, 'elements': tree.elements})\n",
    "\n",
    "nodes = tree.get_all_nodes_to_list()\n",
    "for node in nodes:\n",
    "    if node['elements'] is not None:\n",
    "        sp = ScatterPlot(node['id'],node['sx'],node['ex'],node['width'],node['elements'],global_inter,2,'timestamp')\n",
    "        graphs.append(node)\n",
    "\n",
    "graphs.append({\n",
    "    'type': 'Brush',\n",
    "    'width': pixel_width,\n",
    "    'height': tree.height\n",
    "})\n",
    "await self.on_draw('', graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7eefed1f-a5d0-4d97-896c-b9b65b738633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script name: AIR3268B42_Abnormal_Analyze.escp\n",
    "Author: DengJun Lei(EEEINLD)\n",
    "Date created: April 12, 2023\n",
    "Description: Batch analyze AIR3268B42 abnormal keywords and group them based on file_name,product_name,assigned_name,cmd,keywords, and Abnormal.\n",
    "\"\"\"\n",
    "\n",
    "###################### User Define ###############################\n",
    "product_name = 'AIR3268B42'\n",
    "group = ['product_name','cmd','keywords']\n",
    "# Files directory Location\n",
    "dir_path = 'D:/projects/ericsson_flow/new_files/AIR3268_DL'\n",
    "\n",
    "exps = {\n",
    "    'trx status': {'extract_exps': {\n",
    "                    'timestamp':{'exp':'{}Date: {timestamp},{}', 'cond': \"row['timestamp_abnormal'] = False\"},\n",
    "                    'txPma':{'exp':'{}txPma{}: {txPma},{}', 'cond': \"row['txPma_abnormal'] = False\"},\n",
    "                    'txDpdPma':{'exp':'{}txDpdPma{}: {txDpdPma},{}', 'cond': \"row['txDpdPma_abnormal'] = True if float(row['txPma']['value']) - float(row['txDpdPma']['value']) > 4 else False\"},\n",
    "                    'txPmb':{'exp':'{}txPmb{}: {txPmb},{}', 'cond': \"row['txPmb_abnormal'] = True if float(row['txPma']['value']) - float(row['txPmb']['value']) != 0 else False\"},\n",
    "                    'txTorPmb':{'exp':'{}txTorPmb{}: {txTorPmb},{}', 'cond': \"row['txTorPmb_abnormal'] = True if float(row['txPma']['value']) - float(row['txTorPmb']['value']) > 0.1 else False\"},\n",
    "                    'dpd':{'exp':'{}dpd {}: {dpd},{}', 'cond': \"row['dpd_abnormal'] = True if row['dpd']['value'] in ['off'] else False\"},\n",
    "                    'dpdStateMachine':{'exp':'{}dpdStateMachine{}: {dpdStateMachine},{}', 'cond': \"row['dpdStateMachine_abnormal'] = True if row['dpdStateMachine']['value'] in ['OFF'] else False\"},\n",
    "                    'gainStateMachine':{'exp':'{}gainStateMachine{}: {gainStateMachine},{}', 'cond': \"row['gainStateMachine_abnormal'] = True if row['gainStateMachine']['value'] in ['CtrlGainStateIdle:started', 'ns:stopped'] else False\"},\n",
    "                    'linearizationStateMachine':{'exp':'{}linearizationStateMachine{}: {linearizationStateMachine},{}', 'cond': \"row['linearizationStateMachine_abnormal'] = True if row['linearizationStateMachine']['value'] in ['ns:stopped'] else False\"},\n",
    "                }, 'time_type': 'batch', 'start_exp': 'coli>/fruacc/lhsh {assigned_name} trx status{}', 'end_exp': 'coli>/fruacc/lhsh{}'},\n",
    "    'elog read': {'extract_exps': {\n",
    "                    'LinFault':{'exp':'{}[{timestamp}]{}Lin. fault port{LinFault}', 'cond': \"row['LinFault_abnormal'] = True if row['LinFault'] is not None else False\"},\n",
    "                    'PowerLost':{'exp':'{}[{timestamp}]{}POWER LOST{PowerLost}', 'cond': \"row['PowerLost_abnormal'] = True if row['PowerLost'] is not None else False\"},\n",
    "                    'PsuEnter':{'exp':'{}[{timestamp}]{}PSU enters{PsuEnter}', 'cond': \"row['PsuEnter_abnormal'] = True if row['PsuEnter'] is not None else False\"},\n",
    "                    'JesdLinkFailure':{'exp':'{}[{timestamp}]{}JESD LINK FAILURE{JesdLinkFailure}', 'cond': \"row['JesdLinkFailure_abnormal'] = True if row['JesdLinkFailure'] is not None else False\"},\n",
    "                    'DpdController':{'exp':'{}[{timestamp}]{}#### DPDCONTROLLER{DpdController}', 'cond': \"row['DpdController_abnormal'] = True if row['DpdController'] is not None else False\"},\n",
    "                }, 'time_type': 'embedded', 'start_exp': 'coli>/fruacc/lhsh {assigned_name} elog read{}', 'end_exp': '{}End of log{}'},\n",
    "}\n",
    "\n",
    "map_table_start_mark = ['FRU','LNH','BOARD','ST','FAULT','OPER','MAINT']\n",
    "map_table_end_mark = '---------------------------------------------------------------------'\n",
    "map_table_exp = '{} ;{assigned_name} ;{product_name} {}'\n",
    "origin_data_output = 'D:/projects/ericsson_flow/new_files/AIR3268OriginData.csv'\n",
    "origin_table_output = 'D:/projects/ericsson_flow/new_files/AIR3268OriginTable.csv'\n",
    "pixel_width = 2000\n",
    "env = 'local'\n",
    "\n",
    "# if env == 'local':\n",
    "#     import os\n",
    "#     from utils import *\n",
    "#     from graph import *\n",
    "#     import random\n",
    "#     import warnings\n",
    "#     warnings.filterwarnings(\"ignore\")\n",
    "###################### Execution area ###############################\n",
    "# await self.on_console(msg='Script running...')\n",
    "# extract keywords and generate origin data\n",
    "def abnormal_condition(row, code):\n",
    "    exec(code)\n",
    "    return row\n",
    "\n",
    "async def printk(msg):\n",
    "    if env == 'local':\n",
    "        print(msg)\n",
    "    else:\n",
    "        await self.on_console(msg=msg)\n",
    "    \n",
    "# for file_index, file_name in enumerate(iterate_files_in_directory(dir_path)):\n",
    "#     tables = []\n",
    "#     path = f'{dir_path}\\\\{file_name}'\n",
    "#     await printk(msg=f'Start handle {file_name}.')\n",
    "#     with open(path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "    \n",
    "#     # find map table\n",
    "#     flag = False\n",
    "#     table = {}\n",
    "#     for line in lines:\n",
    "#         if list(set([True if word in line else False  for word in map_table_start_mark]))[0] == True:\n",
    "#             flag = True\n",
    "#         if flag:\n",
    "#             r = parse(map_table_exp, line)\n",
    "#             if r is not None:\n",
    "#                 table[r.named['assigned_name']] = r.named['product_name']\n",
    "#                 tables.append({'file_name': file_name, 'product_name': r.named['product_name'], 'assigned_name': r.named['assigned_name']})\n",
    "#         if (map_table_end_mark in line) & flag:\n",
    "#             break\n",
    "\n",
    "#     # init \n",
    "#     search_contents = {}\n",
    "#     for key in exps.keys():\n",
    "#         search_contents[key] = []\n",
    "    \n",
    "#     # find search content\n",
    "#     for key in exps.keys():\n",
    "#         flag = False\n",
    "#         content = {'lines':[]}\n",
    "#         for index, line in enumerate(lines):\n",
    "#             if flag:\n",
    "#                 r = parse(exps[key]['end_exp'], line)\n",
    "#                 if r is not None:\n",
    "#                     flag = False\n",
    "#                     search_contents[key].append(content)\n",
    "#                     content = {'lines':[]}\n",
    "#                 else:\n",
    "#                     content['lines'].append({'global_index':index, 'text':line})\n",
    "#                     continue\n",
    "#             r = parse(exps[key]['start_exp'], line)\n",
    "#             if r is not None:\n",
    "#                 flag = True\n",
    "#                 content['assigned_name'] = r['assigned_name']\n",
    "#                 content['lines'].append({'global_index':index, 'text':line})\n",
    "    \n",
    "#     # extract key value\n",
    "#     res = []\n",
    "#     for key in search_contents.keys():\n",
    "#         for cmd_batch, content in enumerate(search_contents[key]):\n",
    "#             tmp = {}\n",
    "#             for extract_key in exps[key]['extract_exps'].keys():\n",
    "#                 if extract_key != 'timestamp':\n",
    "#                     tmp[extract_key] = []\n",
    "            \n",
    "#             if exps[key]['time_type'] == 'embedded':\n",
    "#                 for line in content['lines']:\n",
    "#                     for extract_key in exps[key]['extract_exps'].keys():\n",
    "#                         r = parse(exps[key]['extract_exps'][extract_key]['exp'], line['text'])\n",
    "#                         if r is not None:\n",
    "#                             item = {\n",
    "#                                     'file_name':file_name, 'product_name':table[content['assigned_name']], 'assigned_name':content['assigned_name'], \n",
    "#                                     'cmd':key, 'keywords':extract_key, 'global_index': line['global_index'], 'value':'',\n",
    "#                                     'cmd_batch': cmd_batch, 'timestamp': convert_datetime_timestamp(r.named['timestamp']), 'x': random.randint(0, 100), 'y':random.randint(0, 100)\n",
    "#                                     }\n",
    "#                             tmp[extract_key].append(item)\n",
    "#             elif exps[key]['time_type'] == 'batch':\n",
    "#                 timestamp = ''\n",
    "#                 for line in content['lines']:\n",
    "#                     for extract_key in exps[key]['extract_exps'].keys():\n",
    "#                         r = parse(exps[key]['extract_exps'][extract_key]['exp'], line['text'])\n",
    "#                         if r is not None:\n",
    "#                             if 'timestamp' in r.named:\n",
    "#                                 timestamp = convert_datetime_timestamp(r.named['timestamp'])\n",
    "#                                 break\n",
    "#                             item = {\n",
    "#                                     'file_name':file_name, 'product_name':table[content['assigned_name']], 'assigned_name':content['assigned_name'], \n",
    "#                                     'cmd':key, 'keywords':extract_key, 'global_index': line['global_index'], 'value':r.named[extract_key],\n",
    "#                                     'cmd_batch': cmd_batch, 'timestamp': timestamp, 'x': random.randint(0, 100), 'y':random.randint(0, 100)\n",
    "#                                     }\n",
    "#                             tmp[extract_key].append(item)\n",
    "                \n",
    "#             for keyword in tmp.keys():\n",
    "\n",
    "#                 max_len = max([len(v) for v in tmp.values()])\n",
    "#                 for k, v in tmp.items():\n",
    "#                     if len(v) < max_len:\n",
    "#                         v.extend([None] * (max_len - len(v)))\n",
    "        \n",
    "#                 an = keyword+'_abnormal'\n",
    "#                 ptmp = pd.DataFrame(tmp)\n",
    "#                 if len(ptmp) == 0:\n",
    "#                     break\n",
    "#                 ptmp = ptmp.reset_index().rename(columns={'index':'occur_batch'})\n",
    "#                 ptmp = ptmp.apply(lambda x: abnormal_condition(x, exps[key]['extract_exps'][keyword]['cond']), axis=1)\n",
    "#                 abnormal = []\n",
    "#                 for item,occur_batch  in ptmp.loc[(ptmp[an] == True), [keyword, 'occur_batch']].values:\n",
    "#                     item['occur_batch'] = occur_batch\n",
    "#                     abnormal.append(item)\n",
    "#                 res.append({'file_name': file_name, 'product_name': table[content['assigned_name']],'assigned_name': content['assigned_name'], 'cmd': key, 'keywords': keyword, 'abnormal':abnormal})\n",
    "#     await printk(msg=f'Finish count={file_index} {file_name}.')\n",
    "#     if len(res) == 0:\n",
    "#         continue\n",
    "#     pd.DataFrame(res).to_csv(origin_data_output, index=False, mode='a', header=not os.path.exists(origin_data_output))\n",
    "#     pd.DataFrame(tables).to_csv(origin_table_output, index=False, mode='a', header=not os.path.exists(origin_table_output))\n",
    "\n",
    "temp_data = pd.read_csv('D:/projects/ericsson_flow/new_files/AIR3268TempData.csv')\n",
    "temp_data['abnormal'] = temp_data['abnormal'].apply(lambda x: eval(x))\n",
    "origin_data = pd.read_csv('D:/projects/ericsson_flow/new_files/AIR3268OriginData.csv')\n",
    "origin_table = pd.read_csv('D:/projects/ericsson_flow/new_files/AIR3268OriginTable.csv')\n",
    "total_radios = len(origin_table.loc[(origin_table['product_name'] == product_name),:].reset_index(drop=True))\n",
    "\n",
    "# draw picture\n",
    "items = []\n",
    "group.append('abnormal')\n",
    "for p in temp_data[group].values:\n",
    "    groups = [i.replace('.','_') for i in p[0: -1]]\n",
    "    data = p[-1]\n",
    "    ts = []\n",
    "    for dot in data:\n",
    "        if dot['timestamp'] > 1000000000:\n",
    "            ts.append(dot['timestamp'])\n",
    "    items.append({'path': list(groups), 'graph_type': 'ScatterPlot', 'start_x': min(ts) if len(ts) > 0 else 0, 'end_x': max(ts) if len(ts) > 0 else 0, 'elements': data})\n",
    "\n",
    "# define logic range\n",
    "start_x = []\n",
    "end_x = []\n",
    "for item in items:\n",
    "    if (item['start_x'] !=0) & (item['end_x'] !=0):\n",
    "        start_x.append(item['start_x'])\n",
    "        end_x.append(item['end_x'])\n",
    "\n",
    "graphs = []\n",
    "# define global linear scale\n",
    "global_inter = linear_scale([min(start_x), max(end_x)], [0, pixel_width])\n",
    "# define IndentedTree\n",
    "tree = IndentedTree('', 0, pixel_width, pixel_width, items, global_inter)\n",
    "graphs.append({'id': tree.id, 'type': tree.type, 'elements': tree.elements})\n",
    "# define xaxis\n",
    "graphs.append({'type': 'XAxis','width': pixel_width,'lower_bound': min(start_x),'upper_bound': max(end_x),'tick_format_func': 'formatTimestamp'})\n",
    "# define brush\n",
    "graphs.append({'interact_logic':\"\"\"\n",
    "                    that.scriptComponentSvg.controlInteract('statistic', value)\n",
    "                \"\"\"\n",
    "               , 'type': 'Brush','width': pixel_width,'height': tree.height})\n",
    "# define all ScatterPlot\n",
    "nodes = tree.get_all_nodes_to_list()\n",
    "for node in nodes:\n",
    "    if node['elements'] is not None:\n",
    "        sp = ScatterPlot(node['id'],node['sx'],node['ex'],node['width'],node['height'],node['elements'],global_inter,2,'timestamp')\n",
    "        for dot in sp.elements:\n",
    "            dot['timestamp'] = convert_timestamp_datetime(dot['timestamp'])\n",
    "            dot['api'] = f\"\"\"\n",
    "                that.textAnalysisView.fileContainerView.controlNewFile([\"{dir_path}/{dot['file_name']}\"])\n",
    "                setTimeout(function() {{\n",
    "                   that.textAnalysisView.fileContainerView.textFileViews[that.textAnalysisView.fileContainerView.activeTextFileView].textFileOriginalView.controlJump(d)\n",
    "                }}, 500)\n",
    "            \"\"\"\n",
    "        graphs.append(sp.get_vars())\n",
    "\n",
    "async def on_custom_interact(sid, switch, data):\n",
    "    if switch == 'statistic':\n",
    "        if len(data) == 0:\n",
    "            return\n",
    "        sta = pd.DataFrame(data)\n",
    "        sta = sta.groupby(['file_name', 'assigned_name'])['keywords'].apply(lambda x: '&'.join([i for i in x])).reset_index(name='only_occur')\n",
    "        sta = sta.sort_values(by=['only_occur']).reset_index(drop=True)\n",
    "        only_occur = sta.groupby(['only_occur']).size().reset_index(name='counts')\n",
    "        only_occur['rate'] = only_occur['counts'] / total_radios\n",
    "        res = []\n",
    "        res.append({\n",
    "            'name' : 'statistic',\n",
    "            'data' : only_occur.to_dict('records'),\n",
    "            'display_logic' : \"\"\"\n",
    "                var items = data\n",
    "                var table = this.createElementTable()\n",
    "                items.forEach((item, index) => {\n",
    "                    var tr = this.createElementTr()\n",
    "                    var td = this.createElementTd()\n",
    "                    td.innerHTML = index\n",
    "                    tr.appendChild(td)\n",
    "                    if (index===0) {\n",
    "                        var thead = this.createElementTr()\n",
    "                        var th = this.createElementTh()\n",
    "                        th.innerHTML = 'Index'\n",
    "                        thead.appendChild(th)\n",
    "                        Object.keys(item).forEach(key => {\n",
    "                            var th = this.createElementTh()\n",
    "                            th.innerHTML = key\n",
    "                            thead.appendChild(th)\n",
    "                        })\n",
    "                        table.appendChild(thead)\n",
    "                    }\n",
    "                    Object.keys(item).forEach(key => {\n",
    "                        td = this.createElementTd()\n",
    "                        td.innerHTML = item[key]\n",
    "                        tr.appendChild(td)\n",
    "                    })\n",
    "                    table.appendChild(tr) \n",
    "                })\n",
    "                content.appendChild(table)\n",
    "            \"\"\"\n",
    "        })\n",
    "        res.append({\n",
    "            'name' : 'origin',\n",
    "            'data' : sta.to_dict('records'),\n",
    "            'display_logic' : \"\"\"\n",
    "                var items = data\n",
    "                var table = this.createElementTable()\n",
    "                items.forEach((item, index) => {\n",
    "                    var tr = this.createElementTr()\n",
    "                    var td = this.createElementTd()\n",
    "                    td.innerHTML = index\n",
    "                    tr.appendChild(td)\n",
    "                    if (index===0) {\n",
    "                        var thead = this.createElementTr()\n",
    "                        var th = this.createElementTh()\n",
    "                        th.innerHTML = 'Index'\n",
    "                        thead.appendChild(th)\n",
    "                        Object.keys(item).forEach(key => {\n",
    "                            var th = this.createElementTh()\n",
    "                            th.innerHTML = key\n",
    "                            thead.appendChild(th)\n",
    "                        })\n",
    "                        th = this.createElementTh()\n",
    "                        th.innerHTML = 'Jump'\n",
    "                        thead.appendChild(th)\n",
    "                        table.appendChild(thead)\n",
    "                    }\n",
    "                    Object.keys(item).forEach(key => {\n",
    "                        td = this.createElementTd()\n",
    "                        td.innerHTML = item[key]\n",
    "                        tr.appendChild(td)\n",
    "                    })\n",
    "                    var btnTd = this.createElementTd()\n",
    "                    var btn = this.createElementButton('-->')\n",
    "                    btn.style.backgroundColor = 'green'\n",
    "                    btn.style.width = '100%'\n",
    "                    btn.style.height = '20px'\n",
    "                    btn.style.alignItems = 'center' \n",
    "                    btn.onclick = function(){\n",
    "                        that.textAnalysisView.scriptView.controlInteract('drawFile', item)\n",
    "                    }\n",
    "                    btnTd.appendChild(btn)\n",
    "                    tr.appendChild(btnTd)\n",
    "                    table.appendChild(tr)\n",
    "                })\n",
    "                content.appendChild(table)\n",
    "            \"\"\"\n",
    "        })\n",
    "        await self.emit('newTips', res, namespace=self.namespace)\n",
    "    elif switch == 'drawFile':\n",
    "        file_name = data['file_name']\n",
    "        file_data = origin_data.loc[(origin_data['file_name'] == file_name), :].reset_index(drop=True)\n",
    "        group = ['file_name', 'product_name', 'assigned_name', 'cmd', 'keywords']\n",
    "        items = []\n",
    "        group.append('merged_abnormal')\n",
    "        for p in file_data[group].values:\n",
    "            groups = [i.replace('.','_') for i in p[0: -1]]\n",
    "            data = eval(p[-1])\n",
    "            ts = []\n",
    "            for dot in data:\n",
    "                if dot['timestamp'] > 1000000000:\n",
    "                    ts.append(dot['timestamp'])\n",
    "            items.append({'path': list(groups), 'graph_type': 'ScatterPlot', 'start_x': min(ts) if len(ts) > 0 else 0, 'end_x': max(ts) if len(ts) > 0 else 0, 'elements': data})\n",
    "\n",
    "        # define logic range\n",
    "        start_x = []\n",
    "        end_x = []\n",
    "        for item in items:\n",
    "            if (item['start_x'] !=0) & (item['end_x'] !=0):\n",
    "                start_x.append(item['start_x'])\n",
    "                end_x.append(item['end_x'])\n",
    "\n",
    "        graphs = []\n",
    "        # define global linear scale\n",
    "        global_inter = linear_scale([min(start_x), max(end_x)], [0, pixel_width])\n",
    "        # define IndentedTree\n",
    "        tree = IndentedTree('', 0, pixel_width, pixel_width, items, global_inter)\n",
    "        graphs.append({'id': tree.id, 'type': tree.type, 'elements': tree.elements})\n",
    "        # define xaxis\n",
    "        graphs.append({'type': 'XAxis','width': pixel_width,'lower_bound': min(start_x),'upper_bound': max(end_x),'tick_format_func': 'formatTimestamp'})\n",
    "        # # define brush\n",
    "        # graphs.append({'interact_logic':\"\"\"\n",
    "        #                     that.scriptComponentSvg.controlInteract('statistic', value)\n",
    "        #                 \"\"\"\n",
    "        #                , 'type': 'Brush','width': pixel_width,'height': tree.height})\n",
    "        # define all ScatterPlot\n",
    "        nodes = tree.get_all_nodes_to_list()\n",
    "        for node in nodes:\n",
    "            if node['elements'] is not None:\n",
    "                sp = ScatterPlot(node['id'],node['sx'],node['ex'],node['width'],node['height'],node['elements'],global_inter,2,'timestamp')\n",
    "                for dot in sp.elements:\n",
    "                    dot['timestamp'] = convert_timestamp_datetime(dot['timestamp'])\n",
    "                    dot['api'] = f\"\"\"\n",
    "                        that.textAnalysisView.fileContainerView.controlNewFile([\"{dir_path}/{dot['file_name']}\"])\n",
    "                        setTimeout(function() {{\n",
    "                           that.textAnalysisView.fileContainerView.textFileViews[that.textAnalysisView.fileContainerView.activeTextFileView].textFileOriginalView.controlJump(d)\n",
    "                        }}, 500)\n",
    "                    \"\"\"\n",
    "                graphs.append(sp.get_vars())\n",
    "        await self.on_new_graphs('', [{'name': file_name, 'content': graphs}])\n",
    "self.on_interact = on_custom_interact\n",
    "await self.on_new_graphs('', [{'name': 'statistic', 'content': graphs}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37b8c430-4037-4dc0-b293-90c9f042c20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = 'DL-NL17317-1-202303301322.log'\n",
    "origin_data = pd.read_csv('D:/projects/ericsson_flow/new_files/AIR3268OriginData.csv')\n",
    "file_data = origin_data.loc[(origin_data['file_name'] == file_name), :].reset_index(drop=True)\n",
    "\n",
    "group = ['file_name', 'product_name', 'assigned_name', 'cmd', 'keywords']\n",
    "items = []\n",
    "group.append('merged_abnormal')\n",
    "for p in file_data[group].values:\n",
    "    groups = [i.replace('.','_') for i in p[0: -1]]\n",
    "    data = eval(p[-1])\n",
    "    ts = []\n",
    "    for dot in data:\n",
    "        if dot['timestamp'] > 1000000000:\n",
    "            ts.append(dot['timestamp'])\n",
    "    items.append({'path': list(groups), 'graph_type': 'ScatterPlot', 'start_x': min(ts) if len(ts) > 0 else 0, 'end_x': max(ts) if len(ts) > 0 else 0, 'elements': data})\n",
    "\n",
    "# define logic range\n",
    "start_x = []\n",
    "end_x = []\n",
    "for item in items:\n",
    "    if (item['start_x'] !=0) & (item['end_x'] !=0):\n",
    "        start_x.append(item['start_x'])\n",
    "        end_x.append(item['end_x'])\n",
    "\n",
    "graphs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "10e53519-7c0f-4264-985f-2af98ecaffe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "if a == {}:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6a94d47-e932-4cf9-8588-bc860e813537",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mloc[(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m product_name), :]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28meval\u001b[39m(x))\n\u001b[1;32m----> 5\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmerged_abnormal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m956656087\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby(group)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [i \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m j])\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabnormal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m result\n",
      "File \u001b[1;32mD:\\projects\\ericsson_env\\ericsson_toolsets_env\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\ericsson_env\\ericsson_toolsets_env\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\ericsson_env\\ericsson_toolsets_env\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mD:\\projects\\ericsson_env\\ericsson_toolsets_env\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[86], line 5\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mloc[(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m product_name), :]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28meval\u001b[39m(x))\n\u001b[1;32m----> 5\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [x[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m956656087\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[0;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby(group)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_abnormal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [i \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m j])\u001b[38;5;241m.\u001b[39mreset_index(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabnormal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m result\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "group = ['product_name','cmd','keywords']\n",
    "result = pd.read_csv(origin_data_output)\n",
    "result = result.loc[(result['product_name'] == product_name), :].reset_index(drop=True)\n",
    "result['merged_abnormal'] = result['merged_abnormal'].apply(lambda x: eval(x))\n",
    "result['merged_abnormal'] = result['merged_abnormal'].apply(lambda x: [x[0]] if len(x) > 0 else [])\n",
    "result = result.groupby(group)['merged_abnormal'].apply(lambda x: [i for j in x for i in j]).reset_index(name='abnormal')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cd67c84b-84f8-43fb-a573-c15f8e863a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1561"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_table = pd.read_csv('D:/projects/ericsson_flow/new_files/AIR3268OriginTable.csv')\n",
    "total_radios = len(map_table.loc[(map_table['product_name'] == product_name),:].reset_index(drop=True))\n",
    "total_radios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a4d6f-8911-4bd9-99ed-5e829f377d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ericsson_toolsets_env",
   "language": "python",
   "name": "ericsson_toolsets_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
