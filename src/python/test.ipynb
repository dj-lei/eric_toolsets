{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc2e3a-893f-4652-b085-d5bb4c10dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import zlib\n",
    "import base64\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import configparser\n",
    "from utils import *\n",
    "from sys import platform\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# import torch\n",
    "# from tslearn.metrics import dtw, dtw_path\n",
    "# from tslearn.metrics import lcss, lcss_path\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
    "# model = BertModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "# model = model.to(device)\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253b509-184b-441e-9a6c-08d7d625752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = configparser.ConfigParser()\n",
    "cf.read('config/config.cfg')\n",
    "\n",
    "env = 'DEVELOP'\n",
    "if 'win' in platform:\n",
    "    env = 'DEVELOP'\n",
    "elif 'linux' in platform:\n",
    "    env = 'PRODUCT'\n",
    "    \n",
    "class EsCtrl(object):\n",
    "    def __init__(self):\n",
    "        self.es_ctrl = Elasticsearch(cf['ENV_'+env]['ADDR'], ca_certs=cf['ELASTICSEARCH']['CA_CERTS'])\n",
    "\n",
    "    def query_index_logs(self, index):\n",
    "        # query = {\n",
    "        #     \"match\": {\n",
    "        #         \"trace\": \"com_ericsson_trithread:INFO\"\n",
    "        #     }\n",
    "        # }\n",
    "        #data = self.es_ctrl.search(index=index, query=query, scroll='1s', size=10000)\n",
    "        data = self.es_ctrl.search(index=index, scroll='1s', size=10000)\n",
    "        sid = data['_scroll_id']\n",
    "        scroll_size = len(data['hits']['hits'])\n",
    "        res = []\n",
    "        while scroll_size > 0:\n",
    "            # Before scroll, process current batch of hits\n",
    "            res.extend(data['hits']['hits'])\n",
    "            data = self.es_ctrl.scroll(scroll_id=sid, scroll='1s')\n",
    "            # Update the scroll ID\n",
    "            sid = data['_scroll_id']\n",
    "            # Get the number of results that returned in the last scroll\n",
    "            scroll_size = len(data['hits']['hits'])\n",
    "        return res\n",
    "\n",
    "    def query_indices(self):\n",
    "        res = []\n",
    "        for key in self.es_ctrl.indices.get_alias().keys():\n",
    "            if len(key) > 0:\n",
    "                if '.analyzed_' in key:\n",
    "                    res.append(key.replace('.analyzed_', ''))\n",
    "        return res\n",
    "\n",
    "    def is_exists(self, index):\n",
    "        return self.es_ctrl.indices.exists(index=index)\n",
    "\n",
    "    def count_index(self, index):\n",
    "        return self.es_ctrl.count(index=index)['count']\n",
    "\n",
    "    def store_index(self, index, data):\n",
    "        data = deflate_and_base64_encode(json.dumps(data).encode('utf-8'))\n",
    "        return self.es_ctrl.index(index=index, body={'content': data})\n",
    "\n",
    "    def query_index(self, index):\n",
    "        data = self.es_ctrl.search(index=index)\n",
    "        data = json.loads(decode_base64_and_inflate(data['hits']['hits'][0]['_source']['content']))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2c72c-6dd7-4e5f-b00a-861983fc0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Data Clean ################################################\n",
    "def package_kv(df):\n",
    "    res = {}\n",
    "    k_type = {}\n",
    "    for i, (kv,index,timestamp) in enumerate(zip(df.kv.values, df['index'].values, df.timestamp.values)):\n",
    "        for item in kv:\n",
    "            if len(item[1]) > 0:\n",
    "                if item[0] in res:\n",
    "                    res[item[0]].append(item[1]  + [timestamp] + [str(i)] + [index]) # [value1,value2,value3,timestamp,process_index,global_index]\n",
    "                else:\n",
    "                    res[item[0]] = [item[1]  + [timestamp] + [str(i)] + [index]]\n",
    "    for key in res.keys():\n",
    "        width = max(map(len, res[key])) # get max width\n",
    "        type_list = []\n",
    "        for i, item in enumerate(res[key]):\n",
    "            if '0x' in item[0]:\n",
    "                type_list.append('register')\n",
    "            elif item[0].isupper():\n",
    "                type_list.append('discrete')\n",
    "            else:\n",
    "                type_list.append('continuous')\n",
    "                \n",
    "            if len(item) != width:\n",
    "                tmp = [0 for _ in range(0, width)]\n",
    "                tmp[-3] = item[-3]\n",
    "                tmp[-2] = item[-2]\n",
    "                tmp[-1] = item[-1]\n",
    "                res[key][i] = tmp\n",
    "        res[key] = np.array(res[key]).transpose().tolist() # matrix transposition\n",
    "        k_type[key] = 'discrete' if len(set(type_list)) > 1 else list(set(type_list))[0]\n",
    "    return res,k_type\n",
    "\n",
    "\n",
    "def package_inverted_index_table(table, key, data):\n",
    "    def clean_special_symbols(text):\n",
    "        for ch in ['/','*','{','}','[',']','(',')','#','+','-','!','=',':',',','\"']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch,\" \")\n",
    "        return re.sub(\" +\", \" \", text)\n",
    "    for index, msg in data:\n",
    "        for word in set(clean_special_symbols(msg).split(' ')):\n",
    "            w = word.lower()\n",
    "            if w not in table:\n",
    "                table[w] = {'x': [index], 'y': [key]} # x:global index, y: yaxis num \n",
    "            else:\n",
    "                table[w]['x'].append(index)\n",
    "                table[w]['y'].append(key)\n",
    "\n",
    "\n",
    "def clean_data(esdata):\n",
    "    def clean_msg_special_symbols(text):\n",
    "        for ch in ['{', '}', '[', ']', '(', ')', '\"', '::']:\n",
    "            if ch in text:\n",
    "                text = text.replace(ch, \" \")\n",
    "        return re.sub(\" +\", \" \", text)\n",
    "\n",
    "    story = []\n",
    "    for item in esdata:\n",
    "        if 'msg' in item['_source']:\n",
    "            tmp = clean_msg_special_symbols(item['_source']['msg'])\n",
    "            if len(re.findall('process \\= (.*?)$', tmp)) > 0:\n",
    "                process = re.findall('process \\= (.*?),', tmp)[0]\n",
    "                msg = re.findall('msg \\= (.*?)$', tmp)[0]\n",
    "            #                 fileAndLine = re.findall('fileAndLine \\= \\\"(.*?)\\\"',item['_source']['msg'])[0].split(':')[0]\n",
    "            # elif len(re.findall('procname \\= (.*?)$', tmp)) > 0:\n",
    "            #     process = re.findall('procname \\= (.*?),', tmp)[0]\n",
    "            #     #                 msg = tmp.split(',')[2].replace('\"','').replace('}','').replace('{','')\n",
    "            #     msg = tmp\n",
    "            else:\n",
    "                process = 'main'\n",
    "                msg = tmp\n",
    "                \n",
    "            msg = msg.replace('= ',':').replace(' = ',':').replace(': ',':').replace(' : ',':').replace('=',':')\n",
    "\n",
    "            for elm in re.split('[: ]',msg):\n",
    "                if elm.isupper():\n",
    "                    msg = re.sub('[: ]'+elm, ':'+elm, msg)\n",
    "\n",
    "            msg = re.sub('(:(?!-).*?[ $])', r'\\1,', (msg + ' $'))\n",
    "            # msg\n",
    "            kv = []\n",
    "            for k, v in re.findall('([A-Za-z0-9_.]+?)[ ]?[:=][ ]?(.*?)[,$]', msg):\n",
    "                if len(v.strip()) > 0:\n",
    "                    if (v.strip()+'xx').lower()[0:2] == '0x':\n",
    "                        kv.append((k.strip()+'(r)',  [v.strip()]))\n",
    "                    elif v.strip()[0].isalpha():\n",
    "                        kv.append((k.strip()+'(d)', [v.strip()]))\n",
    "                    else:\n",
    "                        kv.append((k.strip()+'(c)', re.findall('[0-9.]+', v)))\n",
    "                        \n",
    "            millisecond = str(item['_source']['millisecond'])\n",
    "            supply_zero = ''\n",
    "            for _ in range(0, 9-len(millisecond)):\n",
    "                supply_zero = supply_zero + '0'\n",
    "            millisecond = supply_zero + millisecond\n",
    "            story.append([item['_source']['device'], item['_source']['trace'], process,  item['_source']['logtime'][:-1] + '.' + millisecond, item['_source']['msg'], kv])\n",
    "            \n",
    "    story = pd.DataFrame(story, columns=['device', 'trace', 'process', 'timestamp', 'msg', 'kv']).sort_values('timestamp',ascending=True).reset_index(drop=True)\n",
    "\n",
    "    story_line = {}\n",
    "    inverted_index_table = {}\n",
    "    for dev in set(story.device.values):\n",
    "        data = story.loc[(story['device'] == dev), :].reset_index(drop=True)\n",
    "        sub_inverted_index_table = {}\n",
    "        for i, process_name in enumerate(sorted(set(data.process.values), key=list(data.process.values).index)):\n",
    "            process = data.loc[(data['process'] == process_name), :].reset_index()\n",
    "            process['index'] = process['index'].astype(str)\n",
    "            process_start_time = process['timestamp'][0]\n",
    "            process_start_count = process['index'][0]\n",
    "            process_end_time = process['timestamp'][process.shape[0] - 1]\n",
    "            process_end_count = process['index'][process.shape[0] - 1]\n",
    "            package_inverted_index_table(sub_inverted_index_table, i, zip(process['index'].values, process.msg.values))\n",
    "            msg = dict(zip(process['index'].values, [str(a) + '||' + b + '||' + c for a, b, c in\n",
    "                                                     zip(process.index.values, process.timestamp.values,\n",
    "                                                         process.msg.values)]))\n",
    "\n",
    "            kv,k_type = package_kv(process)\n",
    "            if dev not in story_line:\n",
    "                story_line[dev] = [{'process': process_name, 'start_time': process_start_time, 'start_count': process_start_count, 'end_time': process_end_time, 'end_count': process_end_count, 'msg': msg, 'kv': kv}]\n",
    "            else:\n",
    "                story_line[dev].append({'process': process_name, 'start_time': process_start_time, 'start_count': process_start_count, 'end_time': process_end_time, 'end_count': process_end_count, 'msg': msg, 'kv': kv})\n",
    "        inverted_index_table[dev] = sub_inverted_index_table\n",
    "    return {'story_line': story_line, 'inverted_index_table': inverted_index_table}\n",
    "\n",
    "\n",
    "def apply_filter_by_keywords(df):\n",
    "    if (len(set(df['msg']) & set([':', '='])) > 0):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_filter_digit(df):\n",
    "    return re.sub('\\d+', '', df['msg'])\n",
    "\n",
    "\n",
    "def apply_keyword_highlight(df, keywords, color_highlight):\n",
    "    tmp = [item.lower() for item in keywords]\n",
    "    for item in tmp:\n",
    "        if (item == 'abn:') & (item in df['msg'].lower()):\n",
    "            return color_highlight\n",
    "        elif len(set(df['msg'].lower().split(' ')).intersection(set(tmp))) > 0:\n",
    "            return color_highlight\n",
    "    return df['status']\n",
    "\n",
    "\n",
    "def cal_time_difference(start, end):\n",
    "    return datetime.datetime.strptime(end, \"%H:%M:%S\") - datetime.datetime.strptime(start, \"%H:%M:%S\")\n",
    "\n",
    "\n",
    "############################################ XML Compression and Decompression ################################################\n",
    "def decode_base64_and_inflate(b64string):\n",
    "    decoded_data = base64.b64decode(b64string)\n",
    "    return zlib.decompress(decoded_data , -15)\n",
    "\n",
    "\n",
    "def deflate_and_base64_encode(string_val):\n",
    "    zlibbed_str = zlib.compress(string_val)\n",
    "    compressed_string = zlibbed_str[2:-4]\n",
    "    return base64.b64encode(compressed_string).decode(\"utf-8\")\n",
    "\n",
    "############################################ Text ecoder ################################################\n",
    "def pretrained_model_encode_msg(object1, object2):\n",
    "    with torch.no_grad():\n",
    "        object1_inputs = tokenizer(list(object1.msg.values), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        object1_outputs = model(**object1_inputs)\n",
    "\n",
    "        object2_inputs = tokenizer(list(object2.msg.values), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        object2_outputs = model(**object2_inputs)\n",
    "    return object1_outputs, object2_outputs\n",
    "\n",
    "def onehot_encode_string(str1,str2):\n",
    "    data = []\n",
    "    data.extend(str1)\n",
    "    data.extend(str2)\n",
    "    values = np.array(data)\n",
    "\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder().fit(values)\n",
    "    integer_encoded = label_encoder.transform(values)\n",
    "\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False).fit(integer_encoded)\n",
    "\n",
    "    encoder1 = onehot_encoder.transform(label_encoder.transform(str1).reshape(len(str1), 1))\n",
    "    encoder2 = onehot_encoder.transform(label_encoder.transform(str2).reshape(len(str2), 1))\n",
    "    # inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "    # print(inverted)\n",
    "    return encoder1,encoder2\n",
    "\n",
    "############################################ Space Vectors Algorithm ################################################\n",
    "def cal_lcss_path_and_score(s_y1, s_y2):\n",
    "    path, score = lcss_path(s_y1, s_y2)\n",
    "    return path, score\n",
    "\n",
    "def cal_dtw_path_and_score(s_y1, s_y2):\n",
    "    path, score = dtw_path(s_y1, s_y2)\n",
    "    return path, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import *\n",
    "\n",
    "with open(cf['ENV_'+env]['LOG_STORE_PATH'] + 'ru_lock_unlock_dpd_hw_fault_air6419_mongoose_2022_10_10', \"rb\") as myfile:\n",
    "    S = myfile.read()\n",
    "res = json.loads(gzip.decompress(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-logistics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tslearn.generators import random_walks\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn import metrics\n",
    "\n",
    "\n",
    "numpy.random.seed(0)\n",
    "n_ts, sz, d = 2, 100, 1\n",
    "dataset = random_walks(n_ts=n_ts, sz=sz, d=d, random_state=5)\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)  # Rescale time series\n",
    "dataset_scaled = scaler.fit_transform(dataset)\n",
    "\n",
    "lcss_path, sim_lcss = metrics.lcss_path(dataset_scaled[0, :, 0], dataset_scaled[1, :40, 0], eps=1.5)\n",
    "dtw_path, sim_dtw = metrics.dtw_path(dataset_scaled[0, :, 0], dataset_scaled[1, :40, 0])\n",
    "\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "plt.plot(dataset_scaled[0, :, 0], \"b-\", label='First time series')\n",
    "plt.plot(dataset_scaled[1, :40, 0], \"g-\", label='Second time series')\n",
    "\n",
    "for positions in lcss_path:\n",
    "    plt.plot([positions[0], positions[1]],\n",
    "             [dataset_scaled[0, positions[0], 0], dataset_scaled[1, positions[1], 0]], color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"Time series matching with LCSS\")\n",
    "\n",
    "plt.figure(2, figsize=(8, 8))\n",
    "plt.plot(dataset_scaled[0, :, 0], \"b-\", label='First time series')\n",
    "plt.plot(dataset_scaled[1, :40, 0], \"g-\", label='Second time series')\n",
    "\n",
    "for positions in dtw_path:\n",
    "    plt.plot([positions[0], positions[1]],\n",
    "             [dataset_scaled[0, positions[0], 0], dataset_scaled[1, positions[1], 0]], color='orange')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Time series matching with DTW\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_a = [[int(item)] for item in story_a['kv']['txlProcBranchH']['txAtt(c)'][0]]\n",
    "kv_b = [[int(item)] for item in story_b['kv']['txlProcBranchH']['txAtt(c)'][0]]\n",
    "dataset = [kv_a, kv_b]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-construction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tslearn.generators import random_walks\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn import metrics\n",
    "\n",
    "\n",
    "# numpy.random.seed(0)\n",
    "# n_ts, sz, d = 2, 100, 1\n",
    "# dataset = random_walks(n_ts=n_ts, sz=sz, d=d, random_state=5)\n",
    "\n",
    "kv_a = [int(item) for item in story_a['kv']['txlProcBranchH']['txAtt(c)'][0]]\n",
    "kv_b = [int(item) for item in story_b['kv']['txlProcBranchH']['txAtt(c)'][0]]\n",
    "[kv_b.insert(0,0) for _ in range(0, len(kv_a) - len(kv_b))]\n",
    "\n",
    "dataset = [kv_a, kv_b]\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0., std=1.)  # Rescale time series\n",
    "dataset_scaled = scaler.fit_transform(dataset)\n",
    "\n",
    "lcss_path, sim_lcss = metrics.lcss_path(dataset_scaled[0, :, 0], dataset_scaled[1, :, 0], eps=1.5)\n",
    "dtw_path, sim_dtw = metrics.dtw_path(dataset_scaled[0, :, 0], dataset_scaled[1, :, 0])\n",
    "\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "plt.plot(dataset_scaled[0, :, 0], \"b-\", label='First time series')\n",
    "plt.plot(dataset_scaled[1, :, 0], \"g-\", label='Second time series')\n",
    "\n",
    "for positions in lcss_path:\n",
    "    plt.plot([positions[0], positions[1]],\n",
    "             [dataset_scaled[0, positions[0], 0], dataset_scaled[1, positions[1], 0]], color='orange')\n",
    "plt.legend()\n",
    "plt.title(\"Time series matching with LCSS\")\n",
    "\n",
    "plt.figure(2, figsize=(8, 8))\n",
    "plt.plot(dataset_scaled[0, :, 0], \"b-\", label='First time series')\n",
    "plt.plot(dataset_scaled[1, :, 0], \"g-\", label='Second time series')\n",
    "\n",
    "for positions in dtw_path:\n",
    "    plt.plot([positions[0], positions[1]],\n",
    "             [dataset_scaled[0, positions[0], 0], dataset_scaled[1, positions[1], 0]], color='orange')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Time series matching with DTW\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5301e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import *\n",
    "\n",
    "path = 'exiosuu_LTE_TALAGAKOCAK_GH_BXP_2053_telog'\n",
    "fe = FileExtract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-convert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe7250-7976-4c1a-879b-bd4b83ad470d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tslearn.metrics import dtw, dtw_path\n",
    "from tslearn.metrics import lcss, lcss_path\n",
    "import numpy\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from extract import *\n",
    "\n",
    "def cal_lcss_path_and_score(s_y1, s_y2):\n",
    "    path, score = lcss_path(s_y1, s_y2)\n",
    "    return path, score\n",
    "\n",
    "def cal_dtw_path_and_score(s_y1, s_y2):\n",
    "    path, score = dtw_path(s_y1, s_y2)\n",
    "    return path, score\n",
    "\n",
    "with open(cf['ENV_'+env]['LOG_STORE_PATH'] + 'GLT_SUKAMULYA_CBN_CM_BXP_2051_telog.log_BXP_2051_radio6626_2022_10_10', \"rb\") as myfile:\n",
    "    S = myfile.read()\n",
    "story_a = json.loads(gzip.decompress(S))\n",
    "\n",
    "with open(cf['ENV_'+env]['LOG_STORE_PATH'] + 'exiosuu_LTE_TALAGAKOCAK_GH_2052.log_BXP_2052_radio6626_2022_10_10', \"rb\") as myfile:\n",
    "    S = myfile.read()\n",
    "story_b = json.loads(gzip.decompress(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-richmond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bisect import bisect\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def del_list_inplace(l, id_to_del):\n",
    "    for i in sorted(id_to_del, reverse=True):\n",
    "        del(l[i])\n",
    "\n",
    "keyword = 'Pma(c)'\n",
    "final = pd.DataFrame()\n",
    "kv_a_global_indices = [int(item) for item in story_a['kv']['txlProcBranchH'][keyword][-1]]\n",
    "kv_b_global_indices = [int(item) for item in story_b['kv']['txlProcBranchH'][keyword][-1]]\n",
    "\n",
    "kv_a = [[i, kv_a_global_indices[i], float(item)] for i, item in enumerate(story_a['kv']['txlProcBranchH'][keyword][0])]\n",
    "kv_b = [[i, kv_b_global_indices[i], float(item)] for i, item in enumerate(story_b['kv']['txlProcBranchH'][keyword][0])]\n",
    "\n",
    "kv_a = pd.DataFrame(kv_a, columns=['x', 'global_index','value'])\n",
    "kv_a['category'] = 'story_a'\n",
    "kv_a['loop'] = 'origin'\n",
    "\n",
    "kv_b = pd.DataFrame(kv_b, columns=['x', 'global_index','value'])\n",
    "kv_b['category'] = 'story_b'\n",
    "kv_b['loop'] = 'origin'\n",
    "\n",
    "final = final.append(kv_a).reset_index(drop=True)\n",
    "final = final.append(kv_b).reset_index(drop=True)\n",
    "\n",
    "for loop,_ in enumerate(range(0, 2)):\n",
    "    path, score = lcss_path(NormalizeData(list(kv_a.value.values)), NormalizeData(list(kv_b.value.values)), eps=0.1)\n",
    "    o_a = [[i, kv_a['global_index'][item[0]], kv_a['value'][item[0]]] for i, item in enumerate(path)]\n",
    "    o_b = [[i, kv_b['global_index'][item[1]], kv_b['value'][item[1]]] for i, item in enumerate(path)]\n",
    "    tmp_a = pd.DataFrame(o_a, columns=['x', 'global_index','value'])\n",
    "    tmp_a['category'] = 'story_a'\n",
    "    tmp_a['loop'] = 'loop'+str(loop)\n",
    "    tmp_b = pd.DataFrame(o_b, columns=['x', 'global_index','value'])\n",
    "    tmp_b['category'] = 'story_b'\n",
    "    tmp_b['loop'] = 'loop'+str(loop)\n",
    "    final = final.append(tmp_a).reset_index(drop=True)\n",
    "    final = final.append(tmp_b).reset_index(drop=True)\n",
    "\n",
    "    kv_a = kv_a.drop(kv_a.index[ [item[0] for item in path] ]).reset_index(drop=True)\n",
    "    kv_b = kv_b.drop(kv_b.index[ [item[1] for item in path] ]).reset_index(drop=True)\n",
    "    \n",
    "sns.relplot(\n",
    "    data=final, kind=\"line\",\n",
    "    x=\"x\", y=\"value\", col=\"category\", hue=\"loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = final.loc[(final['loop'] != 'origin'), :].reset_index(drop=True)\n",
    "sns.relplot(\n",
    "    data=tmp, kind=\"line\",\n",
    "    x=\"x\", y=\"value\", col=\"category\", hue=\"loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=final, kind=\"line\",\n",
    "    x=\"global_index\", y=\"value\", col=\"category\", hue=\"loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = 'txlProcBranchH'\n",
    "\n",
    "highlight_story = {}\n",
    "story_a_error_global_indices = []\n",
    "for i, p in enumerate(story_a['inverted_index_table']['error']['process']):\n",
    "    if p == process:\n",
    "        story_a_error_global_indices.append(int(story_a['inverted_index_table']['error']['x'][i]))\n",
    "        \n",
    "story_b_error_global_indices = []\n",
    "for i, p in enumerate(story_b['inverted_index_table']['error']['process']):\n",
    "    if p == process:\n",
    "        story_b_error_global_indices.append(int(story_b['inverted_index_table']['error']['x'][i]))\n",
    "\n",
    "highlight_story = {'story_a':story_a_error_global_indices, 'story_b':story_b_error_global_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-valve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for loop in set(final.loop.values):\n",
    "    for story in set(final.category.values):\n",
    "        gi = final.loc[(final['loop'] == loop)&(final['category'] == story), :].global_index.values\n",
    "        print(loop, story, len(gi))\n",
    "        tmp = []\n",
    "        for index in highlight_story[story]:\n",
    "            tmp.append(bisect(gi, index))\n",
    "        res.append({'loop':loop, 'category':story, 'position': sorted(set(tmp), key=tmp.index)})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(final.loc[(final['loop'] == 'loop1')&(final['category'] == 'story_a'), :].value.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(final.loc[(final['loop'] == 'loop0')&(final['category'] == 'story_a'), :].value.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-elite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 132\n",
    "b = [0, 10, 30, 60, 100, 150, 210, 280, 340, 480, 530]\n",
    "print(bisect(b, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-staff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.值范围 2.相似线段形态 3.highlight落在区间内 4.方差大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-admission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a0ecd-8513-4eee-b382-718cf750cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_msg_special_symbols(text):\n",
    "    for ch in ['{', '}', '[', ']', '(', ')', '\"', '::']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, \" \")\n",
    "    return re.sub(\" +\", \" \", text)\n",
    "    \n",
    "# msg = 'GaN Boost mode, set to boost mode Pma:-15.88[-41.54 -9.50] dB, DpdPma:-20.09[-20.58 -19.18] dB, Pmb:-15.88, TorPmb:-15.92[-50.88 -9.50] dB, avgTxPma:-18.14 dB, pmDpdIrqStat:0x00000000, pmScaleFactor: 65K'\n",
    "# msg = 'txAtt:145, txAttPeak:0, dpGainLoopEnable:true, dpGainCtrlType:VVA_QPB93, torTemperature:820 (0.1C), torGainBackoff:0 (0.01dB), torGainLin:3.82825(0.01dB), torStepBit:9, cc1Ctrl1=0x00000100 , avgIMpa0:690 [mAmp]'\n",
    "# msg = '[TXL_GAIN] Pma:-inf[-41.54 -9.50] dB, DpdPma:-inf[-inf -inf] dB, Pmb:-inf, TorPmb:-inf[-inf -9.50] dB, avgTxPma:-inf dB, pmDpdIrqStat:0x00000000, pmScaleFactor: 65K'\n",
    "# msg = 'New event= EVENT_DEACTIVATE carrierId= 196908 fbsId= 1 fbsState= DISABLED cycleRequired= YES 110'\n",
    "# msg = 'Event CARRIER_DEACTIVATE for carrierId:778'\n",
    "# msg = 'Set event RX_SETUP_EVENT to time: 250[ms], from 0x13000e3'\n",
    "# msg = 'New event= EVENT_SETUP carrierId= 771 fbsId= 1 fbsState= SETUP cycleRequired= NO 0'\n",
    "# msg = '0-insertion for fbsId=2 event=EVENT_RELEASE '\n",
    "# msg = 'DP trace: 339: 4909172 dllb_radon.c(4780) INFO:7: Status: stat{dpd=0x00808c10 pd{0=0x00808c10, 1=0x00808410} ec=0x000002ff}'\n",
    "msg = 'BXP_2051: [2022-09-27 13:33:42.291419830] (+0.000196450) radio6626 com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"txlProcBranchH\", fileAndLine = \"dpdController.cc:1887\", msg = \"txAtt:339, txAttPeak:0, dpGainLoopEnable:true, dpGainCtrlType:DSA_AD_TXFE, torTemperature:830 (0.1C), torGainBackoff:0 (0.01dB), torGainLin:3.89942(0.01dB), torStepBit:9, cc1Ctrl5=0x00000118 , avgIMpa0:230 [mAmp]\" }'\n",
    "# msg = clean_msg_special_symbols(msg)\n",
    "# msg = msg.replace('= ',':').replace(' = ',':').replace(': ',':').replace(' : ',':').replace('=',':')\n",
    "\n",
    "# for elm in re.split('[: ]',msg):\n",
    "#     if elm.isupper():\n",
    "#         msg = re.sub('[: ]'+elm, ':'+elm, msg)\n",
    "\n",
    "# msg = re.sub('(:(?!-).*?[ $])', r'\\1,', (msg + ' $'))\n",
    "# msg\n",
    "# kv = []\n",
    "# for k, v in re.findall('([A-Za-z0-9_.]+?)[ ]?[:=][ ]?(.*?)[,$]', msg):\n",
    "#     if (v.strip()+'xx').lower()[0:2] == '0x':\n",
    "#         kv.append((k.strip()+'(r)',  [v.strip()]))\n",
    "#     elif v.strip()[0].isalpha():\n",
    "#         kv.append((k.strip()+'(d)', [v.strip()]))\n",
    "#     else:\n",
    "#         kv.append((k.strip()+'(c)', re.findall('[0-9.]+', v)))\n",
    "# kv\n",
    "re.findall('([A-Za-z0-9_.]+?)[ ]?[:=][ ]?(.*?)[,$]', msg)\n",
    "# re.findall('(.*?): \\[(.*?)\\] \\((.*?)\\) (.*?) (.*?): (.*?)$', msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_msg_special_symbols(text):\n",
    "    for ch in ['{', '}', '[', ']', '(', ')', '\"', '::', '\\'']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, \" \")\n",
    "    return re.sub(\" +\", \" \", text)\n",
    "# msg = 'BXP_2051: [Trace log from 2 restarts before]'\n",
    "# msg = 'BXP_2050: [2022-07-14 21:47:47.132] txlProcBranchA dpdController.cc:3069 INFO:txAtt:1812, txAttPeak:0, dpGainLoopEnable:false, dpGainCtrlType:IDLE, torGainLin:2.61517, torStepBit:7, ccCtrl:0x00000100, avgIMpa0:470 [mAmp]'\n",
    "# msg = 'BXP_2051: [2022-07-09 15:53:37.007706415] (+?.?????????) radio6626 com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"txlProcBranchE\", fileAndLine = \"dpdController.cc:2112\", msg = \"Power measurement, Pma:-14.35[-41.54 -9.50] dB, DpdPma:-18.25[-19.05 -17.65] dB, Pmb:-14.35, TorPmb:-14.42[-49.35 -9.50] dB, avgTxPma:-13.62 dB, pmDpdIrqStat:0x00008000, pmScaleFactor: 65K\" }'\n",
    "# msg = '[17:46:31.176471339] (+0.000298760) air6419_mongoose com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"txlProcBranch5\", fileAndLine = \"delayEstGen3Drv.cc:240\", msg = \"Fractional delay ok. IntegerDelay: 0x0000030c, FracDelay: 0x0000003a, dpdIrqStat: 0x04000080, dpdStat: 0x04000080, txSurveyMaxDpdAddr: 0x00000101, delEstIntFracDelta: 0xffffffff, delEstIntCorr: 0x00000000, delEstFracCorr: 0x00002000\" }'\n",
    "msg = '{ cpu_id = 3 }, { process = \"txlProcBranch5\", fileAndLine = \"delayEstGen3Drv.cc:240\", msg = \"Fractional delay ok. IntegerDelay: 0x0000030c, FracDelay: 0x0000003a, dpdIrqStat: 0x04000080, dpdStat: 0x04000080, txSurveyMaxDpdAddr: 0x00000101, delEstIntFracDelta: 0xffffffff, delEstIntCorr: 0x00000000, delEstFracCorr: 0x00002000\" }'\n",
    "\n",
    "msg = clean_msg_special_symbols(msg)\n",
    "# re.findall('(.*?): \\[(.*?)\\] \\((.*?)\\) (.*?) (.*?): (.*?)$', msg)\n",
    "# re.findall('\\[(.*?)\\] \\((.*?)\\) (.*?) (.*?): (.*?)$', msg)\n",
    "re.findall('process \\= (.*?),.*?msg \\= (.*)$', msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "str1 = 'BXP_2: [2022-11-18 12:51:26.550407110] (+0.004950450) radio6626 com_ericsson_trithread:INFO: { cpu_id = 2 }, { process = \"txlProcBranchI\", fileAndLine = \"dpdController.cc:1886\", msg = \"Gain started. Pma:-21.79[-41.54 -9.50] dB, DpdPma:-25.83[-26.49 -25.09] dB, Pmb:-21.79, TorPmb:-21.92[-56.79 -9.50] dB, avgTxPma:-inf dB, pmDpdIrqStat:0x00000000, pmScaleFactor: 65K\" }\\n'\n",
    "regex = \"\\[%{TIMESTAMP:time}\\] %{DROP:tmp}txAtt:%{INT:txAtt}, %{DROP:tmp}avgIMpa0:%{INT:avgIMpa0} \"\n",
    "# regex = \"%{STRING:device}: \\[%{TIMESTAMP:time}\\] \"\n",
    "v_regex = regex\n",
    "for i, r in enumerate(re.findall('%\\{.*?\\}', regex)):\n",
    "    regex = regex.replace(r, '(.*?)')\n",
    "    v_regex = v_regex.replace(r, '<font color=\"color:#FFFFFF\">'+\"\\\\\"+str(i+1)+'</font>')\n",
    "\n",
    "re.findall(regex, str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-durham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word_replace(word):\n",
    "    return word.group(0).replace(word.group(1), '<font color=\"color:#FFFFFF\">'+word.group(1)+'</font>')\n",
    "    \n",
    "special_symbols = ['/','\\*','\\{','\\}','\\[','\\]','\\(','\\)','#','+','-','!','=',':',',','\"','\\'','>','<','@','$','%','^','\\&','\\|',' ']\n",
    "reg = '['+'|'.join(special_symbols)+']' +'(txAtt|txlProcBranchH)'+ '['+'|'.join(special_symbols)+']'\n",
    "re.sub(reg, word_replace,str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub(regex, r''+r'\\1: [\\2] \\\\(\\3\\\\) \\4 \\5: \\6txAtt:\\7, txAttPeak:\\8,\\9torTemperature:\\10 ', str1)\n",
    "re.sub(regex, v_regex, str1).replace('\\\\','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ebc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def udpate_value(item, value):\n",
    "    item['value'] = value\n",
    "    return item\n",
    "tmp = [{'a':1, 'value':3},{'a':1, 'value':4}]\n",
    "list(map(udpate_value, tmp, ['aa', 'aa']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "a.index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_type_correct(_type, reg):\n",
    "    try:\n",
    "        if _type == 'STRING':\n",
    "            return True, reg\n",
    "        elif _type == 'INT':\n",
    "            return True, int(reg)\n",
    "        elif _type == 'FLOAT':\n",
    "            return True, float(reg)\n",
    "        return False, ''\n",
    "    except:\n",
    "        return False, ''\n",
    "\n",
    "test_regex = [\"%{STRING:device}: \\[%{TIMESTAMP:time}\\] \\(%{STRING:cost}\\) %{STRING:name} %{STRING:trace}: %{DROP:tmp}txAtt:%{INT:txAtt}, txAttPeak:%{INT:txAttPeak},%{DROP:tmp}torTemperature:%{INT:torTemperature} \"]\n",
    "\n",
    "key_value = {}\n",
    "key_type = {}\n",
    "key_name = {}\n",
    "time_index = {}\n",
    "regexs = []\n",
    "for n_regex, regex in enumerate(test_regex):\n",
    "    key_type[n_regex] = {}\n",
    "    key_name[n_regex] = {}\n",
    "    for index, item in enumerate(re.findall('%\\{(.*?)\\}', regex)):\n",
    "        if item.split(':')[0] == 'TIMESTAMP':\n",
    "            time_index[n_regex] = index\n",
    "\n",
    "        if (item.split(':')[0] != 'DROP')&(item.split(':')[0] != 'TIMESTAMP'):\n",
    "            key_value[item.split(':')[1]] = []\n",
    "\n",
    "        key_type[n_regex][index] = item.split(':')[0]\n",
    "        key_name[n_regex][index] = item.split(':')[1]\n",
    "\n",
    "    for r in re.findall('%\\{.*?\\}', regex):\n",
    "        regex = regex.replace(r, '(.*?)')\n",
    "    regexs.append(regex)\n",
    "\n",
    "for line in fe.res_search_lines:\n",
    "    for n_regex, regex in enumerate(regexs):\n",
    "        regex_res = re.findall(regex, self.parent.lines[line])[0]\n",
    "        if len(regex_res) > 0:\n",
    "            c_time = regex_res[time_index[n_regex]]\n",
    "            for index, reg in enumerate(regex_res[0]):\n",
    "                flag, value = is_type_correct(key_type[index], reg)\n",
    "                if flag:\n",
    "                    key_value[key_name[index]].append({'value': value, 'timestamp': c_time})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from file_operate import FileOperate\n",
    "\n",
    "time1 = time.time()\n",
    "# path = 'test1.txt'\n",
    "path = 'GLT_SUKAMULYA_CBN_CM_BXP_2051_telog'\n",
    "fe = FileOperate(path)\n",
    "print(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-product",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "time1 = time.time()\n",
    "fe.extract_inverted_index()\n",
    "\n",
    "tmp = {}\n",
    "for i in fe.inverted_index_table_copy.keys():\n",
    "    tmp = ray.get(fe.inverted_index_table_copy[i])\n",
    "    \n",
    "print(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b572bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ray\n",
    "import time\n",
    "\n",
    "num_cpus = 10\n",
    "ray.init(ignore_reinit_error=True, num_cpus=num_cpus)\n",
    "\n",
    "def clean_special_symbols(text, symbol):\n",
    "    for ch in ['/','*','{','}','[',']','(',')','#','+','-','!','=',':',',','\"','\\'','>','<','@','`','$','%','^','&','|']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch,symbol)\n",
    "    return re.sub(symbol+\"+\", symbol, text)\n",
    "\n",
    "@ray.remote\n",
    "def extract(lines):\n",
    "    return_dict= {}\n",
    "    for index, line in enumerate(lines):\n",
    "        for word in set(clean_special_symbols(line,' ').split(' ')):\n",
    "            if len(word) > 0:\n",
    "                if (not word[0].isdigit()) & (word not in ['\\n', ' ']):\n",
    "                    if (word not in return_dict):\n",
    "                        return_dict[word] = [index]\n",
    "                    else:\n",
    "                        return_dict[word].append(index)\n",
    "    return return_dict\n",
    "\n",
    "path = 'D:\\FirstScene\\samples\\GLT_SUKAMULYA_CBN_CM_BXP_2051_telog.log'\n",
    "with open(path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "width = int(len(lines) / num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ceab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "result = []\n",
    "for cpu_n in range(num_cpus):\n",
    "    result.append(extract.remote(lines[cpu_n*width : (cpu_n+1)*width - 1]))\n",
    "inverted_index_table = ray.get(result)\n",
    "# print(inverted_index_table[3].keys())\n",
    "print(time.time() - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-contemporary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def iterate_files_in_directory(directory):\n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    res = []\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            res.append(filename)\n",
    "    return res\n",
    "\n",
    "# _dir = 'ELOG_READ_FULL'\n",
    "# for num, gz in enumerate(iterate_files_in_directory(_dir)):\n",
    "#     print(num, gz)\n",
    "#     path = _dir+'/'+gz\n",
    "#     regex = \"^(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +\"\n",
    "#     with gzip.open(path, 'r') as f:\n",
    "#         lines = [line.decode(\"utf-8\").replace('\\n','') for line in f.readlines()]\n",
    "#         # find device info\n",
    "#         devices = []\n",
    "#         for i, line in enumerate(lines):\n",
    "#             if (len(re.findall(regex, line)) > 0) & ('Radio 6626' in line):\n",
    "#                 j = i\n",
    "#                 while ('coli>' not in lines[j]):\n",
    "#                     if len(re.findall(regex, lines[j])) > 0:\n",
    "#                         devices.append(re.findall(regex, lines[j])[0])\n",
    "#                     j = j + 1\n",
    "#                 break\n",
    "\n",
    "#         # find log range\n",
    "#         flag = False\n",
    "#         res = {}\n",
    "#         for line in lines:\n",
    "#             if ('coli>' in line) & ('lhsh' in line) & ('elog read' in line):\n",
    "#                 flag = True\n",
    "#                 dev = re.findall('lhsh (.*?) elog read', line)[0]\n",
    "#                 res[dev] = []\n",
    "\n",
    "#             if flag:\n",
    "#                 res[dev].append(line)\n",
    "\n",
    "#         devices = pd.DataFrame(devices, columns=['LDN','LINKHANDLER','HWTYPE','PRODUCT_ID','REV','SERIAL','PRODUCTION_DATE'])\n",
    "#         for dev in res.keys():\n",
    "#             tmp = devices.loc[(devices['LINKHANDLER'] == dev),['LINKHANDLER','HWTYPE','PRODUCT_ID','REV','SERIAL','PRODUCTION_DATE']]\n",
    "#             if len(tmp) > 0:\n",
    "#                 f2 = open('save_log/'+path.split('/')[1].split('.')[0]+'-'+'-'.join(tmp.values[0]).replace('/','_'), \"w\")\n",
    "#                 f2.write('\\n'.join(res[dev]))\n",
    "#                 f2.close()\n",
    "#     f.close()\n",
    "    \n",
    "_dir = 'YanLin'\n",
    "for num, gz in enumerate(iterate_files_in_directory(_dir)):\n",
    "    print(num, gz)\n",
    "    path = _dir+'/'+gz\n",
    "    regex = \"^(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +\"\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [line.replace('\\n','') for line in f.readlines()]\n",
    "        # find device info\n",
    "        devices = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if (len(re.findall(regex, line)) > 0) & ('Radio 6626' in line):\n",
    "                j = i\n",
    "                while ('coli>' not in lines[j]):\n",
    "                    if len(re.findall(regex, lines[j])) > 0:\n",
    "                        devices.append(re.findall(regex, lines[j])[0])\n",
    "                    j = j + 1\n",
    "                break\n",
    "\n",
    "        # find log range\n",
    "        flag = False\n",
    "        res = {}\n",
    "        for line in lines:\n",
    "            if ('coli>' in line) & ('lhsh' in line) & ('elog read' in line):\n",
    "                flag = True\n",
    "                dev = re.findall('lhsh (.*?) elog read', line)[0]\n",
    "                res[dev] = []\n",
    "\n",
    "            if flag:\n",
    "                res[dev].append(line)\n",
    "\n",
    "        devices = pd.DataFrame(devices, columns=['LDN','LINKHANDLER','HWTYPE','PRODUCT_ID','REV','SERIAL','PRODUCTION_DATE'])\n",
    "        for dev in res.keys():\n",
    "            tmp = devices.loc[(devices['LINKHANDLER'] == dev),['LINKHANDLER','HWTYPE','PRODUCT_ID','REV','SERIAL','PRODUCTION_DATE']]\n",
    "            if len(tmp) > 0:\n",
    "                f2 = open('save_log/'+path.split('/')[1].split('.')[0]+'-'+'-'.join(tmp.values[0]).replace('/','_'), \"w\")\n",
    "                f2.write('\\n'.join(res[dev]))\n",
    "                f2.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = _dir+'/LTE_CIBURUY_LEBAK_MT.log.gz'\n",
    "regex = \"^(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +(.*?)  +\"\n",
    "with gzip.open(path, 'r') as f:\n",
    "    lines = [line.decode(\"utf-8\").replace('\\n','') for line in f.readlines()]\n",
    "    # find device info\n",
    "    for i, line in enumerate(lines):\n",
    "        if (len(re.findall(regex, line)) > 0) & ('Radio 6626' in line):\n",
    "            devices = []\n",
    "            j = i\n",
    "            while ('coli>' not in lines[j]):\n",
    "                print(lines[j])\n",
    "                if len(re.findall(regex, lines[j])) > 0:\n",
    "                    devices.append(re.findall(regex, lines[j])[0])\n",
    "                j = j + 1\n",
    "            break\n",
    "\n",
    "    # find log range\n",
    "    flag = False\n",
    "    res = {}\n",
    "    for line in lines:\n",
    "        if ('coli>' in line) & ('lhsh' in line) & ('elog read' in line):\n",
    "            flag = True\n",
    "            dev = re.findall('lhsh (.*?) elog read', line)[0]\n",
    "            res[dev] = []\n",
    "\n",
    "        if flag:\n",
    "            res[dev].append(line)\n",
    "\n",
    "    devices = pd.DataFrame(devices, columns=['LDN','LINKHANDLER','HWTYPE','PRODUCT ID','REV','SERIAL','PRODUCTION DATE'])\n",
    "    for dev in res.keys():\n",
    "        tmp = devices.loc[(devices['LINKHANDLER'] == dev),['LINKHANDLER','HWTYPE','REV','SERIAL']]\n",
    "        if len(tmp) > 0:\n",
    "            f2 = open('save_log/'+path.split('/')[1].split('.')[0]+'-'+'-'.join(tmp.values[0]), \"w\")\n",
    "            f2.write('\\n'.join(res[dev]))\n",
    "            f2.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from file import TextFile\n",
    "\n",
    "def iterate_files_in_directory(directory):\n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    res = []\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            res.append(filename)\n",
    "    return res\n",
    "\n",
    "final = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-cheese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_dir = 'save_log2'\n",
    " \n",
    "for num, filename in enumerate(iterate_files_in_directory(_dir)):\n",
    "    print(num, filename)\n",
    "    fe = TextFile(_dir+'/'+filename)\n",
    "    header = filename.split('-')\n",
    "    res = fe.search('test', '', [\"\\[%{TIMESTAMP:time}\\] %{STRING:msg}$\"], ['(ABN & raiseFaultAndRollback) < R13C193', 'R13C193 < (ABN & raiseFaultAndRollback)'], [])\n",
    "    conditions = []\n",
    "    condition1_happen_time = []\n",
    "    condition2_happen_time = []\n",
    "\n",
    "    # condition 1\n",
    "    flag = True\n",
    "    for c in fe.searchs[res].res_condition['(ABN & raiseFaultAndRollback) < R13C193']:\n",
    "        if (c[2] < c[1]):\n",
    "            if (re.findall('\\[(.*?)\\] ', fe.lines[c[2]])[0] >= '221100 000000') | (re.findall('\\[(.*?)\\] ', fe.lines[c[3]])[0] >= '221100 000000'):\n",
    "                condition1_happen_time.append(re.findall('\\[(.*?)\\] ', fe.lines[c[2]])[0])\n",
    "                flag = False\n",
    "    if flag:\n",
    "        conditions.append(False)\n",
    "    else:\n",
    "        conditions.append(True)\n",
    "\n",
    "    # condition 2\n",
    "    flag = True\n",
    "    for c in fe.searchs[res].res_condition['R13C193 < (ABN & raiseFaultAndRollback)']:\n",
    "        if (c[0] < c[3]):\n",
    "            if (re.findall('\\[(.*?)\\] ', fe.lines[c[2]])[0] >= '221100 000000') | (re.findall('\\[(.*?)\\] ', fe.lines[c[3]])[0] >= '221100 000000'):\n",
    "                condition2_happen_time.append(re.findall('\\[(.*?)\\] ', fe.lines[c[3]])[0])\n",
    "                flag = False\n",
    "    if flag:\n",
    "        conditions.append(False)\n",
    "    else:\n",
    "        conditions.append(True)\n",
    "\n",
    "\n",
    "    header.extend(conditions)\n",
    "    header.append('||'.join(list(set(condition1_happen_time))))\n",
    "    header.append('||'.join(list(set(condition2_happen_time))))\n",
    "    final.append(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "_dir = 'save_log2'\n",
    "filename = 'ULTE_MATOA-BXP_5-Radio 6626 66B1 66B3 C-KRC 161 924_1-R2B-EA8B224402-20220831'\n",
    "fe = TextFile(_dir+'/'+filename)\n",
    "header = filename.split('-')\n",
    "res = fe.search('test', '', [\"\\[%{TIMESTAMP:time}\\] %{STRING:msg}$\"], ['(ABN & raiseFaultAndRollback) < R13C193', 'R13C193 < (ABN & raiseFaultAndRollback)'], [])\n",
    "conditions = []\n",
    "condition1_happen_time = []\n",
    "condition2_happen_time = []\n",
    "\n",
    "# condition 1\n",
    "flag = True\n",
    "for c in fe.searchs[res].res_condition['(ABN & raiseFaultAndRollback) < R13C193']:\n",
    "    if (c[2] < c[1]):\n",
    "        if (re.findall('\\[(.*?)\\] ', fe.lines[c[2]])[0] >= '221100 000000') | (re.findall('\\[(.*?)\\] ', fe.lines[c[3]])[0] >= '221100 000000'):\n",
    "            condition1_happen_time.append(re.findall('\\[(.*?)\\] ', fe.lines[c[2]])[0])\n",
    "            flag = False\n",
    "if flag:\n",
    "    conditions.append(False)\n",
    "else:\n",
    "    conditions.append(True)\n",
    "    \n",
    "# condition 2\n",
    "flag = True\n",
    "for c in fe.searchs[res].res_condition['R13C193 < (ABN & raiseFaultAndRollback)']:\n",
    "    if (c[0] < c[3]):\n",
    "        if (re.findall('\\[(.*?)\\] ', fe.lines[c[2]])[0] >= '221100 000000') | (re.findall('\\[(.*?)\\] ', fe.lines[c[3]])[0] >= '221100 000000'):\n",
    "            condition2_happen_time.append(re.findall('\\[(.*?)\\] ', fe.lines[c[3]])[0])\n",
    "            flag = False\n",
    "if flag:\n",
    "    conditions.append(False)\n",
    "else:\n",
    "    conditions.append(True)\n",
    "        \n",
    "        \n",
    "header.extend(conditions)\n",
    "header.append('||'.join(list(set(condition1_happen_time))))\n",
    "header.append('||'.join(list(set(condition2_happen_time))))\n",
    "final.append(header)\n",
    "final = pd.DataFrame(final, columns=['SiteName', 'LINKHANDLER', 'HWTYPE', 'PRODUCT_ID', 'REV', 'SERIAL', 'PRODUCTION_DATE', '(ABN & raiseFaultAndRollback) < R13C193', 'R13C193 < (ABN & raiseFaultAndRollback)', 'Before Update Lin Fault Date', 'After Update Lin Fault Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../GLT_SUKAMULYA_CBN_CM_logfiles.log'\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    lines = np.array(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7803377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from utils import *\n",
    "from file import TextFile\n",
    "\n",
    "# path = 'D:\\\\projects\\\\ericsson_toolsets\\\\src\\\\python\\\\save_log'\n",
    "# files = iterate_files_in_directory(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-carpet",
   "metadata": {},
   "source": [
    "## Theme_Hector_MFBX_I_J_K_L_Simultaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e2887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "branchs = ['txlProcBranchI','txlProcBranchJ','txlProcBranchK','txlProcBranchL']\n",
    "txAtt_range = [255, 2500, 2500, 255]\n",
    "\n",
    "res = []\n",
    "for file in files:\n",
    "    fe = TextFile('', path+'\\\\'+file, 'test', 'single')\n",
    "    print(file)\n",
    "    header = [file]\n",
    "    for index, branch in enumerate(branchs):\n",
    "        is_linearization_fault = fe.search('is_linearization_fault', branch+' & linearization & fault', [], [], [])\n",
    "        if len(fe.searchs[is_linearization_fault].res_search_lines) > 0:\n",
    "            header.append(True)\n",
    "        else:\n",
    "            header.append(False)\n",
    "\n",
    "        is_over_range = fe.search('is_over_range', 'txlProcBranchI & txAtt', \n",
    "                        [\"\\[%{TIMESTAMP:time}\\] %{DROP:tmp}txAtt:%{INT:txAtt}, \",\n",
    "                        \"\\[%{TIMESTAMP:time}\\] \"], [], [])\n",
    "        txatt = pd.DataFrame(fe.searchs[is_over_range].res_kv['txAtt'])\n",
    "        if len(txatt.loc[(txatt['value'] > txAtt_range[index]), :]) > 0:\n",
    "            header.append(True)\n",
    "        else:\n",
    "            header.append(False)\n",
    "    res.append(header)\n",
    "    \n",
    "final = pd.DataFrame(res, columns=['siteName',\n",
    "                                 'txlProcBranchI txAtt > 255', 'txlProcBranchI linearization fault',\n",
    "                                 'txlProcBranchJ txAtt > 2500', 'txlProcBranchJ linearization fault',\n",
    "                                 'txlProcBranchK txAtt > 2500', 'txlProcBranchK linearization fault',\n",
    "                                 'txlProcBranchL txAtt > 255', 'txlProcBranchL linearization fault',\n",
    "                                ])\n",
    "final.to_excel('Hector_MFBX_I_J_K_L_Simultaneous.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statutory-culture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'save_log\\\\10.84.72.164.log'\n",
    "fe = TextFile('', path, 'test', 'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extra-shannon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = fe.search('is_over_range', 'txlProcBranchE & (txAtt | pmb) | (TxBranchCtrlE & linearization & fault)', \n",
    "                [\n",
    "                \"\\[%{TIMESTAMP:time}\\] %{DROP:tmp}txAtt:%{INT:txAtt}, %{DROP:tmp}avgIMpa0:%{INT:avgIMpa0} \",\n",
    "                \"\\[%{TIMESTAMP:time}\\] %{DROP:tmp}, TorPmb:%{FLOAT:TorPmb0}\\[%{FLOAT:TorPmb1} %{FLOAT:TorPmb2}\\] dB, \",\n",
    "                \"\\[%{TIMESTAMP:time}\\] \\(%{STRING:cost\\) \"], [], [['fault', '#333']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5898ddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['to', 'pmScaleFactor', 'com_ericsson_trithread', 'cpu_id', 'radio6626', 'boost', 'txlProcBranchE', 'Boost', 'INFO', 'avgTxPma', 'msg', 'TorPmb', 'process', 'GaN', 'dpdController', 'fileAndLine', 'mode', 'dB', 'Pma', 'set', 'cc', 'Pmb', 'pmDpdIrqStat', 'DpdPma', 'avgIMpa0', 'torTemperature', 'dpGainCtrlType', 'dpGainLoopEnable', 'torGainBackoff', 'cc1Ctrl0', 'torGainLin', 'torStepBit', 'mAmp', 'txAttPeak', 'true', 'DSA_AD_TXFE', 'txAtt', 'normal', 'Meters', 'Power', 'inf', 'Dpd', 'TXL_GAIN', 'for', 'data', 'Wait', 'ramping', 'Start'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe.searchs[test].res_inverted_index_table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288e04e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe.searchs[test].res_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "boxed-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in fe.searchs[test].res_search_lines:\n",
    "#     print(fe.lines[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in fe.searchs[test].res_search_lines:\n",
    "    print(fe.lines[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "368896ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '2022',\n",
       " '11',\n",
       " '13',\n",
       " '04',\n",
       " '31',\n",
       " '37',\n",
       " '365766200',\n",
       " '0',\n",
       " '000025960',\n",
       " 'radio6626',\n",
       " 'com_ericsson_trithread',\n",
       " 'INFO',\n",
       " 'cpu_id',\n",
       " '3',\n",
       " 'process',\n",
       " 'TxBranchCtrlE',\n",
       " 'fileAndLine',\n",
       " 'txChangeCycleHelper',\n",
       " 'cc',\n",
       " '264',\n",
       " 'msg',\n",
       " 'Txl',\n",
       " 'branch',\n",
       " 'E',\n",
       " 'restart',\n",
       " 'due',\n",
       " 'to',\n",
       " 'txL',\n",
       " 'linearization',\n",
       " 'fault',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_special_symbols(text, symbol):\n",
    "    for ch in ['/','*','{','}','[',']','(',')','#','+','-','!','=',';',':',',','.','\"','\\'','>','<','@','`','$','^','&','|','\\n']:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch,symbol)\n",
    "    return re.sub(symbol+\"+\", symbol, text)\n",
    "\n",
    "str1 = '[2022-11-13 04:31:37.365766200] (+0.000025960) radio6626 com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"TxBranchCtrlE\", fileAndLine = \"txChangeCycleHelper.cc:264\", msg = \"Txl branch E restart due to txL linearization fault!\" }'\n",
    "clean_special_symbols(str1,' ').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "returning-energy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2022-11-13 04:31:37.365766200', '+0.000025960')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "str1 = '[2022-11-13 04:31:37.365766200] (+0.000025960) radio6626 com_ericsson_trithread:INFO: { cpu_id = 3 }, { process = \"TxBranchCtrlE\", fileAndLine = \"txChangeCycleHelper.cc:264\", msg = \"Txl branch E restart due to txL linearization fault!\" }'\n",
    "regex = \"\\[%{TIMESTAMP:time}\\] \\(%{STRING:cost}\\) \"\n",
    "# regex = \"\\[%{TIMESTAMP:time}\\] %{DROP:tmp}, TorPmb:%{FLOAT:TorPmb0}\\[%{FLOAT:TorPmb1} %{FLOAT:TorPmb2}\\] dB, \"\n",
    "v_regex = regex\n",
    "for i, r in enumerate(re.findall('%\\{.*?\\}', regex)):\n",
    "    regex = regex.replace(r, '(.*?)')\n",
    "    v_regex = v_regex.replace(r, '<font color=\"color:#FFFFFF\">'+\"\\\\\"+str(i+1)+'</font>')\n",
    "\n",
    "re.findall(regex, str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "is_linearization_fault = fe.search('is_linearization_fault', 'txlProcBranchI & linearization & fault', \n",
    "                [\"%{STRING:device}: \\[%{TIMESTAMP:time}\\] \\(%{STRING:cost}\\) %{STRING:name} %{STRING:trace}: %{DROP:tmp}txAtt:%{INT:txAtt}, txAttPeak:%{INT:txAttPeak},%{DROP:tmp}torTemperature:%{INT:torTemperature} \",\n",
    "                \"%{STRING:device}: \\[%{TIMESTAMP:time}\\] \\(%{STRING:cost}\\) %{STRING:name} %{STRING:trace}: %{DROP:tmp}Pma:%{FLOAT:Pma0}\\[%{FLOAT:Pma1} %{FLOAT:Pma2}\\] dB, \",\n",
    "                \"%{STRING:device}: \\[%{TIMESTAMP:time}\\] \"\n",
    "                ], [], [])\n",
    "print(time.time() - time1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
